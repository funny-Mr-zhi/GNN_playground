> 学习资料：《深度学习》(花书)

# C3：概率与信息论

**概率论使我们能够提出不确定的声明以及在不确定性存在的情况下进行推理，而信息论使我们能够量化概率分布中的不确定性总量。**(**不确定性**的声明、推理、分布的量化)
> 概率可以被看做是**处理不确定性的逻辑扩展**。**逻辑**提供了一套**形式化的规则**，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。而**概率论**提供了一套**形式化规则**，可以在给定一些命题的似然后，计算其它命题为真的似然。(**似然**是关于模型参数的函数！)

**在人工智能领域主要有两种用途**：
* **设计**：概率法则告诉我们AI系统如何推理，据此我们设计一些算法来计算或者估算由概率论导出的公式
* **解释**：我们可以利用概率论和统计从理论上分析我们提出的AI系统的行为


**机器学习为什么要引入不确定性**

在计算及科学中，许多分支处理的实体大部分都是完全确定且必然的。但机器学习大量使用概率论，因为机器学习通常必须处理不确定量，有时可能需要处理随机量(非确定性的)，不确定性和随机性可能来自于多个方面。
* 被建模系统的内在随机性。
* 不完全观测。即使系统本身确定，我们不能观测到所有驱动系统行为的变量时，该系统也会呈现随机性。
* 不完全建模。模型本身要求必须舍弃某些观测信息，如离散化模型尝试描述连续空间。
> 多数情况下，使用一些简单而不确定性的规则要比复杂而确定的规则更为实用，即使真正的规则是确定性的并且我们建模的系统可以足够精确的容纳复杂的规则。(太过于复杂且精确的系统会变得难以理解和维护)

### 概念

**随机变量**：可以随机地取不同值的变量。
> 一个随机变量只是对可能的状态的描述，它必须伴随一个概率分布来指定每个状态的可能性。

**概率分布**：用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性的大小。
> 具体的概率分布描述方式取决于随机变量是离散的还是连续的。
> * 离散型随机变量的概率分布用**概率质量函数**来描述，求和归一
> * 连续性随机变量的概率分布用**概率密度函数**来描述，积分归一


**联合概率分布**：多个变量的概率分布$P(X = x, Y = y)$表示$X=x, Y=y$同时发生，也可以简写为$P(x, y)$

**边缘概率**：已知一个联合概率分布，定义在子集上的概率分布称为边缘概率分布。
> 如，对于离散随机变量，已知x和y的联合分布概率函数时，x的边缘概率求法$ \forall x \in X, P(X = x) = \sum_y P(X=x, Y=y)  $

**条件概率**：在给定其它事件发生时我们关注的事件出现的概率。
$$P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}$$

**独立性**：两个随机变量是互相独立的可以表示为$\forall x\in X, y \in Y, P(X=x, Y=y) = P(X=x)P(Y=y)$。

**条件独立性**：两个随机变量在给定随机变量z时是条件独立的表示为$\forall x\in X, y \in Y, z \in Z, P(X=x, Y=y|Z=z) = P(X=x|Z=z)P(Y=y|Z=z)$

**期望**：当x由P产生，f作用于x时，f(x)的平均值。

以连续变量为例$E_{x\sim p}[f(x)]=\int p(x)f(x)dx$

**方差**：衡量对x依据它的概率分布进行采样时，随机变量x的函数值会呈现多大的差异。

$Var(f(x)) = E[(f(x) - E[f(x)])^2]$

方差的平方根被称为标准差

**协方差**：某种意义上给出了两个变量线性相关的强度以及这些变量的尺度。
$Cov(f(x), g(y))=E[(f(x) - E[f(x)])(g(y)-E[g(y)])]$

> 其它衡量指标如相关系数将每个变量的贡献归一化，为例之衡量变量的相关性而不受各个变量尺度大小的影响。协方差与方差有联系，但实际上时不同的概念

**协防方差矩阵**：随机向量$x \in R^n$的协方差矩阵是一个nxn的矩阵，并且满足$Cov(x)_{i, j} = Cov(x_i, x_j)$，其对角元素是方差

### 常用概率分布
* `Bernoulli`分布，单个二值随机变量的分布。
$$P(X=x)= \phi^x(1-\phi)^{1-x} \quad x\in \{0, 1\}$$

* `Multinoulli`分布，又叫范畴分布。指在具有k个不同状态的单个离散型随机变量的上的分布。

由向量$\textbf{p} \in [0, 1]^{k-1}$参数化，其中每一个分量$p_i$表示为第i种状态的概率，最后的第k个状态的概率可以通过$1 - \textbf{1}^T\textbf{p}$计算出。

> 伯努利和多重伯努利分布足够用来描述他们领域内的任意分布，因为其领域(离散)很简单。当处理连续型变量时，会有不可数无限多的状态，所以任何通过少量参数描述的概率分布都必须在分布上加以严格限制。

* `normal distribution`正态分布，也叫高斯分布。

$$\mathcal{N}(x; \mu, \sigma^2)=\sqrt{\frac{1}{2\pi \sigma^2}} exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$$

> 采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因。
> * 第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。 中心极限定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。
> * 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。

* `multivariate normal distribution`多维正态分布

$$\mathcal{N}(\textbf{x}; \bm\mu, \sum)=\sqrt{\frac{1}{(2\pi)^n det(\sum)}} exp(-\frac{1}{2}(\bm x-\mu)^T(\bm x - \bm\mu))$$

$\sum$给出了分布的协方差矩阵

* `exponential distribution`指数分布。
$$p(x; \lambda) = \lambda\bm1_{x\ge0}exp(-\lambda x)$$

在深度学习中，经常会需要在x=0点处取得边界点的分布，为例实现这一目的，可以使用指数分布

* `Laplace distribution`

$$Laplace(x;\mu, \gamma)=\frac{1}{2\gamma}exp(-\frac{|x-\mu|}{\gamma})$$

允许在任意一点$\mu$处设置概率质量的峰值。

* `Dirac delta function`：概率分布的所有质量都集中在一个点上。
* `empirical distribution`经验分布，由`Dirac delta`函数组成。
$$\hat p(x) = \frac{1}{m}\sum_{i=1}^m \delta(x-x^{(i)})$$
> 对于离散型函数，经验分布可以被定义为一个多重伯努利分布，对于每一个可能的输入，其概率可以简单地设定为在训练集上那个输入的经验频率。

> 当我们在训练集上训练模型时，我们可以认为从这个训练集上得到的经验分
布指明了我们采样来源的分布。关于经验分布另外一种重要的观点是，它是训练数
据的似然最大的那个概率密度函数

* `mixture distribution`混合分布
$$P(x) = \sum_iP(c=i)P(x|c=i)$$

### 其它范畴补充

**似然**

**因果模型**



