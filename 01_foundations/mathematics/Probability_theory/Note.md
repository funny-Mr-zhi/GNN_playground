f> 学习资料：《深度学习》(花书)

# C3：概率与信息论

**概率论使我们能够提出不确定的声明以及在不确定性存在的情况下进行推理，而信息论使我们能够量化概率分布中的不确定性总量。**(**不确定性**的声明、推理、分布的量化)
> 概率可以被看做是**处理不确定性的逻辑扩展**。**逻辑**提供了一套**形式化的规则**，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。而**概率论**提供了一套**形式化规则**，可以在给定一些命题的似然后，计算其它命题为真的似然。(**似然**是关于模型参数的函数！)

**在人工智能领域主要有两种用途**：
* **设计**：概率法则告诉我们AI系统如何推理，据此我们设计一些算法来计算或者估算由概率论导出的公式
* **解释**：我们可以利用概率论和统计从理论上分析我们提出的AI系统的行为


**机器学习为什么要引入不确定性**

在计算及科学中，许多分支处理的实体大部分都是完全确定且必然的。但机器学习大量使用概率论，因为机器学习通常必须处理不确定量，有时可能需要处理随机量(非确定性的)，不确定性和随机性可能来自于多个方面。
* 被建模系统的内在随机性。
* 不完全观测。即使系统本身确定，我们不能观测到所有驱动系统行为的变量时，该系统也会呈现随机性。
* 不完全建模。模型本身要求必须舍弃某些观测信息，如离散化模型尝试描述连续空间。
> 多数情况下，使用一些简单而不确定性的规则要比复杂而确定的规则更为实用，即使真正的规则是确定性的并且我们建模的系统可以足够精确的容纳复杂的规则。(太过于复杂且精确的系统会变得难以理解和维护)

### 概念

**随机变量**：可以随机地取不同值的变量。
> 一个随机变量只是对可能的状态的描述，它必须伴随一个概率分布来指定每个状态的可能性。

**概率分布**：用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性的大小。
> 具体的概率分布描述方式取决于随机变量是离散的还是连续的。
> * 离散型随机变量的概率分布用**概率质量函数**来描述，求和归一
> * 连续性随机变量的概率分布用**概率密度函数**来描述，积分归一


**联合概率分布**：多个变量的概率分布$P(X = x, Y = y)$表示$X=x, Y=y$同时发生，也可以简写为$P(x, y)$

**边缘概率**：已知一个联合概率分布，定义在子集上的概率分布称为边缘概率分布。
> 如，对于离散随机变量，已知x和y的联合分布概率函数时，x的边缘概率求法$ \forall x \in X, P(X = x) = \sum_y P(X=x, Y=y)  $

**条件概率**：在给定其它事件发生时我们关注的事件出现的概率。
$$P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}$$

**独立性**：两个随机变量是互相独立的可以表示为$\forall x\in X, y \in Y, P(X=x, Y=y) = P(X=x)P(Y=y)$。

**条件独立性**：两个随机变量在给定随机变量z时是条件独立的表示为$\forall x\in X, y \in Y, z \in Z, P(X=x, Y=y|Z=z) = P(X=x|Z=z)P(Y=y|Z=z)$

**期望**：当x由P产生，f作用于x时，f(x)的平均值。

以连续变量为例$E_{x\sim p}[f(x)]=\int p(x)f(x)dx$

**方差**：衡量对x依据它的概率分布进行采样时，随机变量x的函数值会呈现多大的差异。

$Var(f(x)) = E[(f(x) - E[f(x)])^2]$

方差的平方根被称为标准差

**协方差**：某种意义上给出了两个变量线性相关的强度以及这些变量的尺度。
$Cov(f(x), g(y))=E[(f(x) - E[f(x)])(g(y)-E[g(y)])]$

> 其它衡量指标如相关系数将每个变量的贡献归一化，为例之衡量变量的相关性而不受各个变量尺度大小的影响。协方差与方差有联系，但实际上时不同的概念

**协防方差矩阵**：随机向量$x \in R^n$的协方差矩阵是一个nxn的矩阵，并且满足$Cov(x)_{i, j} = Cov(x_i, x_j)$，其对角元素是方差

### 常用概率分布
* `Bernoulli`分布，单个二值随机变量的分布。
$$P(X=x)= \phi^x(1-\phi)^{1-x} \quad x\in \{0, 1\}$$

* `Multinoulli`分布，又叫范畴分布。指在具有k个不同状态的单个离散型随机变量的上的分布。

由向量$\textbf{p} \in [0, 1]^{k-1}$参数化，其中每一个分量$p_i$表示为第i种状态的概率，最后的第k个状态的概率可以通过$1 - \textbf{1}^T\textbf{p}$计算出。

> 伯努利和多重伯努利分布足够用来描述他们领域内的任意分布，因为其领域(离散)很简单。当处理连续型变量时，会有不可数无限多的状态，所以任何通过少量参数描述的概率分布都必须在分布上加以严格限制。

* `normal distribution`正态分布，也叫高斯分布。

$$\mathcal{N}(x; \mu, \sigma^2)=\sqrt{\frac{1}{2\pi \sigma^2}} exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$$

> 采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因。
> * 第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。 中心极限定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。
> * 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。

* `multivariate normal distribution`多维正态分布

$$\mathcal{N}(\textbf{x}; \bm\mu, \sum)=\sqrt{\frac{1}{(2\pi)^n det(\sum)}} exp(-\frac{1}{2}(\bm x-\mu)^T(\bm x - \bm\mu))$$

$\sum$给出了分布的协方差矩阵

* `exponential distribution`指数分布。
$$p(x; \lambda) = \lambda\bm1_{x\ge0}exp(-\lambda x)$$

在深度学习中，经常会需要在x=0点处取得边界点的分布，为例实现这一目的，可以使用指数分布

* `Laplace distribution`

$$Laplace(x;\mu, \gamma)=\frac{1}{2\gamma}exp(-\frac{|x-\mu|}{\gamma})$$

允许在任意一点$\mu$处设置概率质量的峰值。

* `Dirac delta function`：概率分布的所有质量都集中在一个点上。
* `empirical distribution`经验分布，由`Dirac delta`函数组成。
$$\hat p(x) = \frac{1}{m}\sum_{i=1}^m \delta(x-x^{(i)})$$
> 对于离散型函数，经验分布可以被定义为一个多重伯努利分布，对于每一个可能的输入，其概率可以简单地设定为在训练集上那个输入的经验频率。

> 当我们在训练集上训练模型时，我们可以认为从这个训练集上得到的经验分
布指明了我们采样来源的分布。关于经验分布另外一种重要的观点是，它是训练数
据的似然最大的那个概率密度函数

* `mixture distribution`混合分布
$$P(x) = \sum_iP(c=i)P(x|c=i)$$

### 常用函数的有用性质

**Logistic sigmoid**

$$\sigma(x) = \frac{1}{1+exp(-x)}$$

> `logistic sigmoid`的值域为(0, 1)，通常用来产生`Bernoulli`分布中的参数$\phi$

选择原因见[证明](/01_foundations/mathematics/MathPricipleProof.md)

**Softplus**

$$\zeta(x) = log(1 + exp(x))$$

> `softplus`值域为(0, 正无穷)，可以用来产生正态分布的$\beta$和$\sigma$参数。其名称来源于它是函数$x^+ = max(0, x)$的平滑形式。

**重要性质**

$\sigma(x) = \frac{e^x}{e^x+e^0}\\
...$

### 贝叶斯规则

$P(x|y) = \frac{P(x)P(y|x)}{p(y)}$

### 信息论

> 信息论的基本想法是：一个不太可能发生的事件居然发生了，要比一个非常可能发生的事件发生，能提供更多的信息。我们希望通过一种方式量化信息，其需要满足三个性质：1. 非常可能发生的事件信息量少，确定事件没有信息量。2. 较不可能发生的事件具有更高的信息量。3. 独立事件具有增量信息。

>为了满足上述三个性质，定义事件$X = x$的**自信息**为

$$I(x) = -logP(x)$$

底为e时单位为奈特，底为2时单位为香农或比特。

> 自信息只处理单个的输出，我们用**香农熵**来对整个概率分布中的不确定信息总量进行量化

$$H(x) = E_{x \sim P}[I(x)] = -E_{x\sim P}[logP(x)]$$

接近确定性的分布具有较低的熵，接近均匀的分布具有较高的熵。

> 对于同一个随即变量x，有两个独立的概率分布P(x)和Q(x)，可以用**KL散度**来衡量这两个分布的差异。


### 其它范畴补充

**似然**

**因果模型**

## 番外

> 一些有趣的实际问题，引出背后概率与分布相关的数学原理

### 对数世界

一条有趣的定律：
> **本福特定律**，也称为本福特法则，说明从一堆实际生活中得出的数据中，以1为首的数字出现的概率约为总数的三成，为接近直觉得出的期望值$\frac{1}{9}$的三倍。
>
> $首尾数字d概率：P(d) = log_{10}(1+\frac{1}{d})$

或许，我们的世界是一个**对数世界**。[参考：这个定律，预言了你的人生进度条](https://www.bilibili.com/video/BV1VrVSz1Eme/?share_source=copy_web&vd_source=83d993d64b9308b8d03347e513f9825c)

>用一些可靠统计数据和主管感知来说明这条定律。

#### 统计数据

忽略量级，仅记录数据中第一个出现的非零数字，部分实际生活数据如下：

领域|1|2|3|4|5|6|7|8|9
--|--|--|--|--|--|--|--|--|--
杭州超市的商品价格|30.00%|21.00%|7.00%|5.00%|12.00%|8.00%|3.00%|4.00%|10.00%
随机200个视频点赞量|28.5%|19.5%|16.00%|10.00%|3.50%|8.00%|6.00%|6.00%|2.50%
世界国家/地区面积(平方公里)|28.33%|19.31%|13.30%|9.01%|6.87%|5.58%|6.01%|5.15%|6.01%
世界国家/地区面积(平方英里)|30.19|15.88%|16.31%|9.44%|4.29%|6.01%|6.01%|4.72%|4.72%
宇宙天体间距离|31.70%|21.60%|12.60%|8.50%|6.70%|5.60%|4.90%|4.77%|3.63%
斐波那契数列|30.15%|17.59%|12.56%|9.05%|8.54%|6.03%|5.53%|6.03%|4.52%
100种有机物饱和蒸汽压|32.29%|27.69%|21.28%|16.18%|4.71%|5.45%|7.69%|7.29%|1.12%
物理常数(230个)|35.19%|19.74%|9.87%|7.73%|9.01%|6.01%|3.00%|4.29%|5.15%
A股某天股价|31.44%|17.33%|11.35%|9.37%|8.14%|6.29%|5.80%|5.50%|4.79%

> 是不是很神奇，生活中毫不相关的领域为何会同时具有如此反直觉的规律。以上数据都具有一个共同的特点，分布不均匀，有些样本差距会横跨好几个数量级，数学语言讲就是接近对数均匀分布。
当我们对这些数据取log，就会发现它们的分布好像还挺均匀的。
数学上很容易证明对符合数均匀分布的数据第一位是1的概率就是30%左右，但为什么自然中会出现如此多的符合对数均匀分布的数据，毕导的回到是：“不知道”。但本文下面会有讲解。

#### 感知

为了适应处处是对数的世界，我们人类的感知习惯也采用了对数解码的设定。你是否有过以下感觉
* 小时候时间过得特别慢，可以按小时计算，长大以后感觉一眨眼一年，十年就过去了。
* 一个商品单价从1元涨到2元你觉得涨了好多，但如果从10000涨到10001你觉得没有变化

> 很明显，我们对世界的感受不是变化的绝对值，而是变化的相对比例。

**韦伯-费希纳定律**用严格的数学公式表达了这种感觉的特点。

$$\Delta p = k\frac{\Delta S}{S}\quad（韦伯定律）$$

其中$\Delta p$为某量的主观的感觉变化，$\Delta S$为某量的实质变化。对韦伯定律积分，我们就得到了费希纳定律。

$$p = kln(\frac{S}{S_0})$$

**原来人的感觉和物理量的对数成正比。**

许多物理量在对数基础上构建也就不奇怪了
* 物理学声音单位为分贝$L_B = 10log_{10}(\frac{P_1}{P_0})$
* 化学PH值：$pH = -log_{10}[H^+]$
* 地质学里氏震级：$M_L = 10log_{10}(\frac{B}{B_0})$
* 天文学视星(定义星星亮度)：$m = -2.5log_{10}(\frac{F}{F_0})$

或许人脑在许多情况下就是在用对数解码我们对于物理世界的实际输入,形成我们独特的感知。

> 解释上面年龄越大时间过得越快的感知，如果你相信人脑在一些情况下是对数感知世界的话，不妨看看这个例子：
> 
> 如果我们能活到80岁，按照绝对值来衡量，生命的中点当然是40岁。但假如我们按照对数来衡量，以4岁为起点，主观感知下生命的中点其实是18岁。

感知中点即到两端的感知距离相同：
$$log(x) - log(4) = log(80) - log(x)\\
log(x) + log(x) = log(80) + log(4)\\
x^2 = 80 \times 4\\
x = 17.89 \approx 18岁$$

> 这的确挺神奇的，大自然存在如此之多的对数分布，而我们的大脑又用对数运算将它们压缩回可感知的空间中，才让我们体会到，世界在每个尺度上专属的精彩。


**知觉赋予感知以意义，因此知觉产生的是对世界的解释，而不是对世界的完美表征**
 <div style="text-align: right;">
  <font>--菲利普·津巴多《普通心理学》</font>
</div>

### 数学解释

为什么世界上有一些数据分布是对数的，另一些则明显不是，比如我们的身高，手机号码的分布。

**其实问题的答案很简单，只是加法和乘法的不同罢了。**

根据中心极限定理，在做出最小假设的情况下，用常规的正态分布去拟合一个概率分布是最优的。生活中很多自然的分布都与正态分布形状相似。但这里有一个前提假设常常被忽略，那就是数据的**变化模式**。

如果数据的变化模式是**加性**的，即增长和减小都在一个固定常数附近，和数据当前的大小没有关系。那么数据分布很可能接近正太分布。但如果数据的变化模式是**乘性**的呢？即数据的增长和减小都与数据当前的数值大小相关，其分布又该如何衡量。
(如果说加性数据的变化规律是"+1"或"-1"，那么乘性数据的变化规律就是"$\times$2"或"$\times\frac{1}{2}$"。)

 乘性分布其实也很好解决，只需要将乘法计算转化成加法计算，问题就会迎刃而解。这时$log$就排上用场了。我们都学过如果一个数值是乘性增长的，我们把它转化成log值，其自然而然就变成了一个加性增长的量。

  $$log(a) + log(b) = log(a\times b)$$

其实这两种增长也对应了两种分布:
* 轻尾分布：对应加性数据的分布，典型是正态分布。数据大量集中在一个区间，超出该区间概率急剧下降
* 长尾分布：对应乘性数据的分布，典型是对数正态分布，即对原数值取对数后符合正太分布。大多数数据集中在一段区间，但超出该区间，概率下降会越来越慢。

### 总结

生活中乘性数据很常见，比如上面统计数据中的`A股价格`,股价都是按照百分比浮动的，当然是乘性数据，符合本福特定律；`斐波那契数列`，下一个数值的产生于前两个数值，其增长是与当前量的大小有关的，而非一个固定的增量。当然上述的数据中有一些不那么直观的乘性增长的例子，但其统计结果表明它们的确是符合本福特定律的，也就很可能是一个乘性变化的数值。其表现出的统计规律或许能帮助我们更好地透过表象发现其背后的底层逻辑。

>问题留给后来的读者吧。国家/地区面积符合本福特定律，它的变化是乘性的，这一点稍微思考一下就能理解；超市商品价格也符合本福特定律，这说明了什么？(商品价格背后的变化逻辑和分布需要从统计规律来看，但打工人的工资不需要，很明显是加性的。会不会我们拥有着乘性的感知，买着乘性的商品，却挣着加性的死工资，于是活成了为钱财奔波劳作还不知满足的牛马--玩笑)




