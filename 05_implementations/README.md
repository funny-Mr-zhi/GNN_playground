# ğŸ’» ä»£ç å®ç° (Implementations)

é€šè¿‡ä»é›¶å¼€å§‹çš„ä»£ç å®ç°åŠ æ·±å¯¹GNNç®—æ³•çš„ç†è§£ï¼Œæå‡ç¼–ç¨‹èƒ½åŠ›å’Œå·¥ç¨‹å®è·µæ°´å¹³ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ä»é›¶å®ç°ç»å…¸GNNç®—æ³•ï¼Œæ·±å…¥ç†è§£å…¶åŸç†
- æŒæ¡é«˜æ•ˆçš„ç¼–ç¨‹æŠ€å·§å’Œæœ€ä½³å®è·µ
- å­¦ä¼šæ€§èƒ½ä¼˜åŒ–å’Œæ¨¡å‹è°ƒè¯•
- å»ºç«‹å®Œæ•´çš„å®éªŒè¯„ä¼°ä½“ç³»

## ğŸ“ ç›®å½•ç»“æ„

### ğŸ”§ from_scratch/
ä»é›¶å¼€å§‹å®ç°ï¼ŒåŒ…æ‹¬ï¼š
- **åŸºç¡€ç»„ä»¶**ï¼šå›¾æ•°æ®ç»“æ„ã€æ¶ˆæ¯ä¼ é€’æ¡†æ¶
- **ç»å…¸æ¨¡å‹**ï¼šGCNã€GATã€GraphSAGEã€GINç­‰
- **è®­ç»ƒå·¥å…·**ï¼šä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ã€è¯„ä¼°æŒ‡æ ‡
- **å®Œæ•´ç¤ºä¾‹**ï¼šç«¯åˆ°ç«¯çš„è®­ç»ƒå’Œæµ‹è¯•æµç¨‹

### ğŸ“š tutorials/
æ•™ç¨‹å¼ä»£ç ï¼ŒåŒ…æ‹¬ï¼š
- **å…¥é—¨æ•™ç¨‹**ï¼šå¾ªåºæ¸è¿›çš„å­¦ä¹ ææ–™
- **è¿›é˜¶æ•™ç¨‹**ï¼šå¤æ‚æ¨¡å‹å’ŒæŠ€æœ¯çš„å®ç°
- **æ¡ˆä¾‹ç ”ç©¶**ï¼šå…·ä½“é—®é¢˜çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
- **æœ€ä½³å®è·µ**ï¼šä»£ç è§„èŒƒå’Œä¼˜åŒ–æŠ€å·§

### ğŸ“Š benchmarks/
æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
- **æ ‡å‡†æ•°æ®é›†**ï¼šåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„æµ‹è¯•ç»“æœ
- **æ¨¡å‹å¯¹æ¯”**ï¼šä¸åŒGNNæ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ
- **æ•ˆç‡åˆ†æ**ï¼šè®­ç»ƒæ—¶é—´å’Œå†…å­˜ä½¿ç”¨åˆ†æ
- **å¯é‡ç°å®éªŒ**ï¼šå®Œæ•´çš„å®éªŒé…ç½®å’Œç»“æœ

## ğŸ“– å®ç°è®¡åˆ’

### ç¬¬23-25å‘¨ï¼šåŸºç¡€ç»„ä»¶å®ç°

#### ç¬¬23å‘¨ï¼šå›¾æ•°æ®ç»“æ„å’ŒåŸºç¡€æ“ä½œ
```python
# graph.py - åŸºç¡€å›¾æ•°æ®ç»“æ„å®ç°
import numpy as np
import torch
from typing import List, Tuple, Optional, Union

class Graph:
    """åŸºç¡€å›¾æ•°æ®ç»“æ„"""
    
    def __init__(self, num_nodes: int, edge_index: torch.Tensor, 
                 node_features: Optional[torch.Tensor] = None,
                 edge_features: Optional[torch.Tensor] = None):
        self.num_nodes = num_nodes
        self.edge_index = edge_index  # [2, num_edges]
        self.num_edges = edge_index.size(1)
        self.node_features = node_features
        self.edge_features = edge_features
        
        # è®¡ç®—åº¦æ•°
        self.degrees = self._compute_degrees()
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        self.adj_matrix = self._build_adjacency_matrix()
        
    def _compute_degrees(self) -> torch.Tensor:
        """è®¡ç®—èŠ‚ç‚¹åº¦æ•°"""
        degrees = torch.zeros(self.num_nodes, dtype=torch.long)
        degrees.scatter_add_(0, self.edge_index[0], torch.ones_like(self.edge_index[0]))
        return degrees
        
    def _build_adjacency_matrix(self) -> torch.Tensor:
        """æ„å»ºé‚»æ¥çŸ©é˜µ"""
        adj = torch.zeros(self.num_nodes, self.num_nodes)
        adj[self.edge_index[0], self.edge_index[1]] = 1
        return adj
        
    def add_self_loops(self):
        """æ·»åŠ è‡ªç¯"""
        self_loop_index = torch.arange(self.num_nodes).unsqueeze(0).repeat(2, 1)
        self.edge_index = torch.cat([self.edge_index, self_loop_index], dim=1)
        self.num_edges = self.edge_index.size(1)
        
    def normalize_adjacency(self, method: str = 'symmetric'):
        """å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ"""
        if method == 'symmetric':
            # D^(-1/2) * A * D^(-1/2)
            deg = self.degrees.float()
            deg_inv_sqrt = deg.pow(-0.5)
            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
            
            norm = deg_inv_sqrt[self.edge_index[0]] * deg_inv_sqrt[self.edge_index[1]]
            return norm
        else:
            raise ValueError(f"Unknown normalization method: {method}")
            
    def get_neighbors(self, node_id: int) -> List[int]:
        """è·å–èŠ‚ç‚¹çš„é‚»å±…"""
        mask = self.edge_index[0] == node_id
        neighbors = self.edge_index[1][mask].tolist()
        return neighbors
```

#### ç¬¬24å‘¨ï¼šæ¶ˆæ¯ä¼ é€’æ¡†æ¶
```python
# message_passing.py - é€šç”¨æ¶ˆæ¯ä¼ é€’æ¡†æ¶
import torch
import torch.nn as nn
from abc import ABC, abstractmethod
from typing import Dict, Any

class MessagePassing(nn.Module, ABC):
    """é€šç”¨æ¶ˆæ¯ä¼ é€’åŸºç±»"""
    
    def __init__(self, aggr: str = 'add', flow: str = 'source_to_target'):
        super().__init__()
        self.aggr = aggr
        self.flow = flow
        
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # 1. è®¡ç®—æ¶ˆæ¯
        messages = self.message(x, edge_index, edge_attr)
        
        # 2. èšåˆæ¶ˆæ¯
        aggregated = self.aggregate(messages, edge_index)
        
        # 3. æ›´æ–°èŠ‚ç‚¹ç‰¹å¾
        updated = self.update(x, aggregated)
        
        return updated
        
    @abstractmethod
    def message(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """è®¡ç®—è¾¹ä¸Šçš„æ¶ˆæ¯"""
        pass
        
    def aggregate(self, messages: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """èšåˆæ¶ˆæ¯"""
        num_nodes = edge_index.max().item() + 1
        
        if self.aggr == 'add':
            out = torch.zeros(num_nodes, messages.size(1), device=messages.device)
            out.scatter_add_(0, edge_index[1].unsqueeze(1).expand_as(messages), messages)
        elif self.aggr == 'mean':
            out = torch.zeros(num_nodes, messages.size(1), device=messages.device)
            count = torch.zeros(num_nodes, 1, device=messages.device)
            out.scatter_add_(0, edge_index[1].unsqueeze(1).expand_as(messages), messages)
            count.scatter_add_(0, edge_index[1].unsqueeze(1), torch.ones_like(messages[:, :1]))
            out = out / (count + 1e-8)
        elif self.aggr == 'max':
            out = torch.zeros(num_nodes, messages.size(1), device=messages.device)
            out.scatter_reduce_(0, edge_index[1].unsqueeze(1).expand_as(messages), 
                              messages, reduce='amax', include_self=False)
        else:
            raise ValueError(f"Unknown aggregation method: {self.aggr}")
            
        return out
        
    def update(self, x: torch.Tensor, aggregated: torch.Tensor) -> torch.Tensor:
        """æ›´æ–°èŠ‚ç‚¹ç‰¹å¾"""
        return aggregated
```

#### ç¬¬25å‘¨ï¼šè®­ç»ƒå·¥å…·å’Œè¯„ä¼°æŒ‡æ ‡
```python
# trainer.py - è®­ç»ƒå·¥å…·
import torch
import torch.nn.functional as F
from torch.optim import Adam
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import time
from typing import Dict, List, Tuple

class GNNTrainer:
    """GNNè®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, lr: float = 0.01, 
                 weight_decay: float = 5e-4, device: str = 'cpu'):
        self.model = model.to(device)
        self.optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        self.device = device
        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}
        
    def train_epoch(self, data: Graph, train_mask: torch.Tensor) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.optimizer.zero_grad()
        
        out = self.model(data)
        loss = F.cross_entropy(out[train_mask], data.y[train_mask])
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
        
    def evaluate(self, data: Graph, mask: torch.Tensor) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        with torch.no_grad():
            out = self.model(data)
            pred = out.argmax(dim=1)
            
            loss = F.cross_entropy(out[mask], data.y[mask]).item()
            acc = accuracy_score(data.y[mask].cpu(), pred[mask].cpu())
            f1 = f1_score(data.y[mask].cpu(), pred[mask].cpu(), average='macro')
            
        return {'loss': loss, 'accuracy': acc, 'f1_score': f1}
        
    def train(self, data: Graph, train_mask: torch.Tensor, 
              val_mask: torch.Tensor, epochs: int = 200, 
              early_stopping: int = None, verbose: bool = True) -> Dict[str, List]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_acc = 0
        patience_counter = 0
        
        for epoch in range(epochs):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(data, train_mask)
            
            # éªŒè¯
            val_metrics = self.evaluate(data, val_mask)
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            
            # æ—©åœæ£€æŸ¥
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                patience_counter += 1
                
            if early_stopping and patience_counter >= early_stopping:
                if verbose:
                    print(f"Early stopping at epoch {epoch}")
                break
                
            if verbose and epoch % 10 == 0:
                print(f"Epoch {epoch:03d}: Train Loss: {train_loss:.4f}, "
                      f"Val Loss: {val_metrics['loss']:.4f}, "
                      f"Val Acc: {val_metrics['accuracy']:.4f}, "
                      f"Time: {time.time() - start_time:.2f}s")
                      
        return self.history
```

### ç¬¬26-28å‘¨ï¼šç»å…¸æ¨¡å‹å®ç°

#### ç¬¬26å‘¨ï¼šGCNå®ç°
```python
# gcn.py - Graph Convolutional Network
import torch
import torch.nn as nn
import torch.nn.functional as F
from message_passing import MessagePassing

class GCNConv(MessagePassing):
    """GCNå·ç§¯å±‚"""
    
    def __init__(self, in_channels: int, out_channels: int, 
                 bias: bool = True, normalize: bool = True):
        super().__init__(aggr='add')
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.normalize = normalize
        
        self.linear = nn.Linear(in_channels, out_channels, bias=bias)
        
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        # æ·»åŠ è‡ªç¯
        num_nodes = x.size(0)
        self_loop_index = torch.arange(num_nodes, device=edge_index.device).unsqueeze(0).repeat(2, 1)
        edge_index = torch.cat([edge_index, self_loop_index], dim=1)
        
        # è®¡ç®—å½’ä¸€åŒ–ç³»æ•°
        if self.normalize:
            edge_weight = self._compute_edge_weight(edge_index, num_nodes)
        else:
            edge_weight = torch.ones(edge_index.size(1), device=edge_index.device)
            
        return super().forward(x, edge_index, edge_weight)
        
    def message(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: torch.Tensor) -> torch.Tensor:
        # çº¿æ€§å˜æ¢
        x = self.linear(x)
        # å‘é€æ¶ˆæ¯æ—¶åŠ æƒ
        return x[edge_index[0]] * edge_attr.unsqueeze(1)
        
    def _compute_edge_weight(self, edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:
        """è®¡ç®—è¾¹çš„å½’ä¸€åŒ–æƒé‡ D^(-1/2) * A * D^(-1/2)"""
        # è®¡ç®—åº¦æ•°
        degree = torch.zeros(num_nodes, device=edge_index.device, dtype=torch.float)
        degree.scatter_add_(0, edge_index[0], torch.ones_like(edge_index[0], dtype=torch.float))
        
        # è®¡ç®— D^(-1/2)
        deg_inv_sqrt = degree.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        
        # è®¡ç®—è¾¹æƒé‡
        edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]
        return edge_weight

class GCN(nn.Module):
    """å®Œæ•´çš„GCNæ¨¡å‹"""
    
    def __init__(self, in_channels: int, hidden_channels: int, 
                 out_channels: int, num_layers: int = 2, dropout: float = 0.5):
        super().__init__()
        
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(in_channels, hidden_channels))
        
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
            
        self.convs.append(GCNConv(hidden_channels, out_channels))
        
        self.dropout = dropout
        
    def forward(self, data: Graph) -> torch.Tensor:
        x, edge_index = data.node_features, data.edge_index
        
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            
        x = self.convs[-1](x, edge_index)
        return x
```

#### ç¬¬27å‘¨ï¼šGATå®ç°
```python
# gat.py - Graph Attention Network
import torch
import torch.nn as nn
import torch.nn.functional as F
from message_passing import MessagePassing

class GATConv(MessagePassing):
    """GATå·ç§¯å±‚"""
    
    def __init__(self, in_channels: int, out_channels: int, heads: int = 1,
                 dropout: float = 0.0, concat: bool = True, bias: bool = True):
        super().__init__(aggr='add')
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.heads = heads
        self.dropout = dropout
        self.concat = concat
        
        # çº¿æ€§å˜æ¢
        self.linear = nn.Linear(in_channels, heads * out_channels, bias=False)
        
        # æ³¨æ„åŠ›å‚æ•°
        self.att_src = nn.Parameter(torch.randn(1, heads, out_channels))
        self.att_dst = nn.Parameter(torch.randn(1, heads, out_channels))
        
        if bias and concat:
            self.bias = nn.Parameter(torch.randn(heads * out_channels))
        elif bias and not concat:
            self.bias = nn.Parameter(torch.randn(out_channels))
        else:
            self.register_parameter('bias', None)
            
        self.reset_parameters()
        
    def reset_parameters(self):
        nn.init.xavier_uniform_(self.linear.weight)
        nn.init.xavier_uniform_(self.att_src)
        nn.init.xavier_uniform_(self.att_dst)
        if self.bias is not None:
            nn.init.zeros_(self.bias)
            
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        # çº¿æ€§å˜æ¢
        x = self.linear(x).view(-1, self.heads, self.out_channels)
        
        return super().forward(x, edge_index)
        
    def message(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è®¡ç®—æ³¨æ„åŠ›ç³»æ•°
        alpha = self._compute_attention(x, edge_index)
        alpha = F.dropout(alpha, p=self.dropout, training=self.training)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        return x[edge_index[0]] * alpha.unsqueeze(-1)
        
    def _compute_attention(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—æ³¨æ„åŠ›ç³»æ•°"""
        # è®¡ç®—æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹çš„æ³¨æ„åŠ›å¾—åˆ†
        alpha_src = (x * self.att_src).sum(dim=-1)[edge_index[0]]  # [num_edges, heads]
        alpha_dst = (x * self.att_dst).sum(dim=-1)[edge_index[1]]  # [num_edges, heads]
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        alpha = alpha_src + alpha_dst
        alpha = F.leaky_relu(alpha, negative_slope=0.2)
        
        # åœ¨æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…ä¸Šè¿›è¡Œsoftmax
        alpha = self._softmax(alpha, edge_index[1])
        
        return alpha
        
    def _softmax(self, src: torch.Tensor, index: torch.Tensor) -> torch.Tensor:
        """åœ¨æŒ‡å®šç»´åº¦ä¸Šè¿›è¡Œsoftmax"""
        num_nodes = index.max().item() + 1
        out = src - src.max()  # æ•°å€¼ç¨³å®šæ€§
        out = out.exp()
        
        # è®¡ç®—åˆ†æ¯
        out_sum = torch.zeros(num_nodes, src.size(1), device=src.device)
        out_sum.scatter_add_(0, index.unsqueeze(1).expand_as(out), out)
        
        # å½’ä¸€åŒ–
        out = out / (out_sum[index] + 1e-16)
        
        return out
        
    def update(self, x: torch.Tensor, aggregated: torch.Tensor) -> torch.Tensor:
        """æ›´æ–°èŠ‚ç‚¹ç‰¹å¾"""
        if self.concat:
            out = aggregated.view(-1, self.heads * self.out_channels)
        else:
            out = aggregated.mean(dim=1)
            
        if self.bias is not None:
            out = out + self.bias
            
        return out

class GAT(nn.Module):
    """å®Œæ•´çš„GATæ¨¡å‹"""
    
    def __init__(self, in_channels: int, hidden_channels: int, 
                 out_channels: int, heads: int = 8, dropout: float = 0.6):
        super().__init__()
        
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, 
                            dropout=dropout, concat=True)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, 
                            heads=1, dropout=dropout, concat=False)
        
        self.dropout = dropout
        
    def forward(self, data: Graph) -> torch.Tensor:
        x, edge_index = data.node_features, data.edge_index
        
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        
        x = self.conv2(x, edge_index)
        
        return x
```

## ğŸ§ª å®éªŒå’Œè°ƒè¯•

### æ¨¡å‹è°ƒè¯•æŠ€å·§

#### 1. æ¢¯åº¦æ£€æŸ¥
```python
def check_gradients(model, data, loss_fn):
    """æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸"""
    model.train()
    output = model(data)
    loss = loss_fn(output, data.y)
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    for name, param in model.named_parameters():
        if param.grad is None:
            print(f"No gradient for {name}")
        elif torch.isnan(param.grad).any():
            print(f"NaN gradient for {name}")
        elif (param.grad == 0).all():
            print(f"Zero gradient for {name}")
```

#### 2. æ¿€æ´»å€¼ç›‘æ§
```python
class ActivationMonitor:
    """ç›‘æ§æ¿€æ´»å€¼çš„ç»Ÿè®¡ä¿¡æ¯"""
    
    def __init__(self):
        self.activations = {}
        
    def register_hooks(self, model):
        """æ³¨å†Œé’©å­å‡½æ•°"""
        for name, module in model.named_modules():
            if isinstance(module, (nn.ReLU, nn.ELU, GCNConv, GATConv)):
                module.register_forward_hook(self._make_hook(name))
                
    def _make_hook(self, name):
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                self.activations[name] = {
                    'mean': output.mean().item(),
                    'std': output.std().item(),
                    'min': output.min().item(),
                    'max': output.max().item(),
                    'zeros': (output == 0).float().mean().item()
                }
        return hook
        
    def print_stats(self):
        """æ‰“å°æ¿€æ´»å€¼ç»Ÿè®¡"""
        for name, stats in self.activations.items():
            print(f"{name}: mean={stats['mean']:.4f}, std={stats['std']:.4f}, "
                  f"min={stats['min']:.4f}, max={stats['max']:.4f}, "
                  f"zeros={stats['zeros']:.2%}")
```

### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 1. å†…å­˜ä¼˜åŒ–
```python
def optimize_memory_usage(model, data):
    """å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""
    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()
    
    with torch.cuda.amp.autocast():
        output = model(data)
        loss = F.cross_entropy(output[data.train_mask], data.y[data.train_mask])
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    
    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()
```

#### 2. è®¡ç®—ä¼˜åŒ–
```python
def sparse_matrix_multiply(adj_matrix, features):
    """ç¨€ç–çŸ©é˜µä¹˜æ³•ä¼˜åŒ–"""
    if adj_matrix.is_sparse:
        return torch.sparse.mm(adj_matrix, features)
    else:
        return torch.mm(adj_matrix, features)
```

## ğŸ“Š åŸºå‡†æµ‹è¯•

### æ ‡å‡†æ•°æ®é›†æµ‹è¯•
```python
# benchmark.py - åŸºå‡†æµ‹è¯•
import time
import torch
from torch_geometric.datasets import Planetoid, TUDataset
from sklearn.model_selection import StratifiedKFold

class GNNBenchmark:
    """GNNæ¨¡å‹åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, models: Dict[str, nn.Module], datasets: List[str]):
        self.models = models
        self.datasets = datasets
        self.results = {}
        
    def run_node_classification(self, dataset_name: str) -> Dict:
        """èŠ‚ç‚¹åˆ†ç±»åŸºå‡†æµ‹è¯•"""
        dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)
        data = dataset[0]
        
        results = {}
        for model_name, model_class in self.models.items():
            print(f"Testing {model_name} on {dataset_name}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            model = model_class(
                in_channels=dataset.num_features,
                hidden_channels=64,
                out_channels=dataset.num_classes
            )
            
            # è®­ç»ƒå’Œè¯„ä¼°
            trainer = GNNTrainer(model)
            history = trainer.train(data, data.train_mask, data.val_mask, epochs=200)
            
            # æµ‹è¯•é›†è¯„ä¼°
            test_metrics = trainer.evaluate(data, data.test_mask)
            
            results[model_name] = {
                'test_accuracy': test_metrics['accuracy'],
                'test_f1': test_metrics['f1_score'],
                'training_time': history['training_time']
            }
            
        return results
        
    def run_graph_classification(self, dataset_name: str, cv_folds: int = 10) -> Dict:
        """å›¾åˆ†ç±»åŸºå‡†æµ‹è¯•"""
        dataset = TUDataset(root=f'/tmp/{dataset_name}', name=dataset_name)
        
        results = {}
        for model_name, model_class in self.models.items():
            print(f"Testing {model_name} on {dataset_name} with {cv_folds}-fold CV")
            
            cv_scores = []
            skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            
            for fold, (train_idx, test_idx) in enumerate(skf.split(range(len(dataset)), 
                                                                   [data.y.item() for data in dataset])):
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                train_loader = DataLoader([dataset[i] for i in train_idx], batch_size=32, shuffle=True)
                test_loader = DataLoader([dataset[i] for i in test_idx], batch_size=32, shuffle=False)
                
                # åˆå§‹åŒ–æ¨¡å‹
                model = model_class(
                    in_channels=dataset.num_features,
                    hidden_channels=64,
                    out_channels=dataset.num_classes
                )
                
                # è®­ç»ƒå’Œè¯„ä¼°
                fold_acc = self._train_graph_classification(model, train_loader, test_loader)
                cv_scores.append(fold_acc)
                
            results[model_name] = {
                'mean_accuracy': np.mean(cv_scores),
                'std_accuracy': np.std(cv_scores),
                'cv_scores': cv_scores
            }
            
        return results
```

## ğŸ¯ å­¦ä¹ æ£€æŸ¥ç‚¹

### å®ç°èƒ½åŠ›æ£€æŸ¥
- [ ] èƒ½å¤Ÿä»é›¶å®ç°æ¶ˆæ¯ä¼ é€’æ¡†æ¶
- [ ] æŒæ¡ç»å…¸GNNæ¨¡å‹çš„æ ¸å¿ƒç®—æ³•
- [ ] å…·å¤‡è°ƒè¯•å’Œä¼˜åŒ–æ¨¡å‹çš„èƒ½åŠ›
- [ ] èƒ½å¤Ÿè¿›è¡Œå®Œæ•´çš„å®éªŒè¯„ä¼°

### å·¥ç¨‹å®è·µæ£€æŸ¥
- [ ] éµå¾ªè‰¯å¥½çš„ä»£ç è§„èŒƒå’Œç»“æ„
- [ ] å…·å¤‡æ€§èƒ½ä¼˜åŒ–çš„æ„è¯†å’ŒæŠ€èƒ½
- [ ] èƒ½å¤Ÿè¿›è¡Œç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•
- [ ] æŒæ¡æ¨¡å‹éƒ¨ç½²çš„åŸºæœ¬æŠ€èƒ½

### ç†è®ºç†è§£æ£€æŸ¥
- [ ] æ·±å…¥ç†è§£å„ç§GNNç®—æ³•çš„åŸç†
- [ ] èƒ½å¤Ÿåˆ†æç®—æ³•çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦
- [ ] ç†è§£ä¸åŒè®¾è®¡é€‰æ‹©çš„å½±å“
- [ ] å…·å¤‡æ”¹è¿›ç°æœ‰ç®—æ³•çš„èƒ½åŠ›

é€šè¿‡ä»é›¶å¼€å§‹çš„ä»£ç å®ç°ï¼Œä½ å°†å¯¹GNNç®—æ³•æœ‰æœ€æ·±å…¥çš„ç†è§£ï¼Œä¸ºåç»­çš„åˆ›æ–°ç ”ç©¶å’Œå®é™…åº”ç”¨å¥ å®šåšå®çš„æŠ€æœ¯åŸºç¡€ï¼
