# ğŸš€ é¡¹ç›®å®è·µ (Projects)

é€šè¿‡å®é™…é¡¹ç›®åº”ç”¨å›¾ç¥ç»ç½‘ç»œè§£å†³çœŸå®ä¸–ç•Œé—®é¢˜ï¼Œæå‡å®æˆ˜èƒ½åŠ›å’Œé¡¹ç›®ç»éªŒã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡GNNåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„åº”ç”¨æ–¹æ³•
- å­¦ä¼šå®Œæ•´çš„é¡¹ç›®å¼€å‘æµç¨‹
- ç§¯ç´¯è§£å†³å®é™…é—®é¢˜çš„ç»éªŒ
- å»ºç«‹ç«¯åˆ°ç«¯çš„ç³»ç»Ÿå¼€å‘èƒ½åŠ›

## ğŸ“ ç›®å½•ç»“æ„

### ğŸ¯ node_classification/
èŠ‚ç‚¹åˆ†ç±»é¡¹ç›®ï¼ŒåŒ…æ‹¬ï¼š
- **ç¤¾äº¤ç½‘ç»œåˆ†æ**ï¼šç”¨æˆ·ç”»åƒã€ç¤¾åŒºå‘ç°
- **çŸ¥è¯†å›¾è°±**ï¼šå®ä½“åˆ†ç±»ã€å…³ç³»é¢„æµ‹
- **ç”Ÿç‰©ç½‘ç»œ**ï¼šè›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹ã€åŸºå› åˆ†æ
- **å­¦æœ¯ç½‘ç»œ**ï¼šè®ºæ–‡åˆ†ç±»ã€ä½œè€…è¯†åˆ«

### ğŸ“Š graph_classification/
å›¾åˆ†ç±»é¡¹ç›®ï¼ŒåŒ…æ‹¬ï¼š
- **åˆ†å­æ€§è´¨é¢„æµ‹**ï¼šè¯ç‰©å‘ç°ã€æ¯’æ€§é¢„æµ‹
- **ç¤¾äº¤ç½‘ç»œåˆ†æ**ï¼šç½‘ç»œç±»å‹è¯†åˆ«
- **ä»£ç åˆ†æ**ï¼šç¨‹åºæ¼æ´æ£€æµ‹
- **è„‘ç½‘ç»œåˆ†æ**ï¼šç–¾ç—…è¯Šæ–­

### ğŸ”— link_prediction/
é“¾æ¥é¢„æµ‹é¡¹ç›®ï¼ŒåŒ…æ‹¬ï¼š
- **æ¨èç³»ç»Ÿ**ï¼šå•†å“æ¨èã€å¥½å‹æ¨è
- **çŸ¥è¯†å›¾è°±è¡¥å…¨**ï¼šç¼ºå¤±å…³ç³»é¢„æµ‹
- **ç”Ÿç‰©ç½‘ç»œ**ï¼šè›‹ç™½è´¨ç›¸äº’ä½œç”¨é¢„æµ‹
- **äº¤é€šç½‘ç»œ**ï¼šè·¯å¾„è§„åˆ’ä¼˜åŒ–

### ğŸŒ real_world_applications/
çœŸå®ä¸–ç•Œåº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š
- **æ™ºèƒ½æ¨èç³»ç»Ÿ**ï¼šç”µå•†ã€å†…å®¹æ¨è
- **é‡‘èé£æ§**ï¼šåæ¬ºè¯ˆã€ä¿¡ç”¨è¯„ä¼°
- **æ™ºæ…§åŸå¸‚**ï¼šäº¤é€šä¼˜åŒ–ã€èµ„æºé…ç½®
- **åŒ»ç–—å¥åº·**ï¼šç–¾ç—…è¯Šæ–­ã€è¯ç‰©å‘ç°

## ğŸ“– é¡¹ç›®å¼€å‘è®¡åˆ’

### ç¬¬29-31å‘¨ï¼šåŸºç¡€ä»»åŠ¡é¡¹ç›®

#### ç¬¬29å‘¨ï¼šèŠ‚ç‚¹åˆ†ç±» - ç¤¾äº¤ç½‘ç»œç”¨æˆ·ç”»åƒ
**é¡¹ç›®ç›®æ ‡**: åŸºäºç¤¾äº¤ç½‘ç»œæ•°æ®é¢„æµ‹ç”¨æˆ·çš„å…´è¶£æ ‡ç­¾

**æ•°æ®é›†**: Facebookã€Redditã€Twitterç­‰ç¤¾äº¤ç½‘ç»œæ•°æ®

**æŠ€æœ¯æ ˆ**:
- æ•°æ®å¤„ç†: NetworkX, Pandas
- æ¨¡å‹: GCN, GAT, GraphSAGE
- è¯„ä¼°: Accuracy, F1-score, AUC

```python
# user_profiling.py - ç”¨æˆ·ç”»åƒé¡¹ç›®ä¸»æ–‡ä»¶
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, GATConv
from sklearn.metrics import classification_report
import pandas as pd
import networkx as nx

class UserProfilingGNN(torch.nn.Module):
    """ç”¨æˆ·ç”»åƒGNNæ¨¡å‹"""
    
    def __init__(self, num_features, num_classes, hidden_dim=64):
        super().__init__()
        self.conv1 = GCNConv(num_features, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.classifier = torch.nn.Linear(hidden_dim, num_classes)
        self.dropout = torch.nn.Dropout(0.5)
        
    def forward(self, x, edge_index):
        # ç¬¬ä¸€å±‚å›¾å·ç§¯
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒå±‚å›¾å·ç§¯
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        
        # åˆ†ç±»å±‚
        x = self.classifier(x)
        return F.log_softmax(x, dim=1)

class SocialNetworkDataProcessor:
    """ç¤¾äº¤ç½‘ç»œæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_path):
        self.data_path = data_path
        
    def load_facebook_data(self):
        """åŠ è½½Facebookæ•°æ®é›†"""
        # åŠ è½½è¾¹åˆ—è¡¨
        edges = pd.read_csv(f"{self.data_path}/facebook_edges.csv")
        edge_index = torch.tensor(edges.values.T, dtype=torch.long)
        
        # åŠ è½½èŠ‚ç‚¹ç‰¹å¾
        features = pd.read_csv(f"{self.data_path}/facebook_features.csv")
        x = torch.tensor(features.drop('user_id', axis=1).values, dtype=torch.float)
        
        # åŠ è½½æ ‡ç­¾
        labels = pd.read_csv(f"{self.data_path}/facebook_labels.csv")
        y = torch.tensor(labels['interest_category'].values, dtype=torch.long)
        
        return Data(x=x, edge_index=edge_index, y=y)
        
    def create_train_test_split(self, data, train_ratio=0.6, val_ratio=0.2):
        """åˆ›å»ºè®­ç»ƒéªŒè¯æµ‹è¯•é›†åˆ’åˆ†"""
        num_nodes = data.x.size(0)
        indices = torch.randperm(num_nodes)
        
        train_size = int(num_nodes * train_ratio)
        val_size = int(num_nodes * val_ratio)
        
        train_mask = torch.zeros(num_nodes, dtype=torch.bool)
        val_mask = torch.zeros(num_nodes, dtype=torch.bool)
        test_mask = torch.zeros(num_nodes, dtype=torch.bool)
        
        train_mask[indices[:train_size]] = True
        val_mask[indices[train_size:train_size+val_size]] = True
        test_mask[indices[train_size+val_size:]] = True
        
        data.train_mask = train_mask
        data.val_mask = val_mask
        data.test_mask = test_mask
        
        return data

def train_user_profiling_model():
    """è®­ç»ƒç”¨æˆ·ç”»åƒæ¨¡å‹"""
    # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    processor = SocialNetworkDataProcessor("data/facebook")
    data = processor.load_facebook_data()
    data = processor.create_train_test_split(data)
    
    # æ¨¡å‹åˆå§‹åŒ–
    model = UserProfilingGNN(
        num_features=data.x.size(1),
        num_classes=len(torch.unique(data.y)),
        hidden_dim=64
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(200):
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            # éªŒè¯
            model.eval()
            with torch.no_grad():
                pred = model(data.x, data.edge_index).argmax(dim=1)
                val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean()
                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}')
            model.train()
    
    # æœ€ç»ˆæµ‹è¯•
    model.eval()
    with torch.no_grad():
        pred = model(data.x, data.edge_index).argmax(dim=1)
        test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()
        print(f'Test Accuracy: {test_acc:.4f}')
        
        # è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
        print(classification_report(
            data.y[data.test_mask].cpu().numpy(),
            pred[data.test_mask].cpu().numpy()
        ))

if __name__ == "__main__":
    train_user_profiling_model()
```

#### ç¬¬30å‘¨ï¼šå›¾åˆ†ç±» - åˆ†å­æ€§è´¨é¢„æµ‹
**é¡¹ç›®ç›®æ ‡**: é¢„æµ‹åˆ†å­çš„ç”Ÿç‰©æ´»æ€§å’Œæ¯’æ€§

**æ•°æ®é›†**: MUTAG, PROTEINS, IMDB-BINARYç­‰

```python
# molecular_property_prediction.py
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool
from torch_geometric.data import DataLoader
from torch_geometric.datasets import TUDataset
from sklearn.model_selection import StratifiedKFold
import numpy as np

class MolecularGNN(torch.nn.Module):
    """åˆ†å­æ€§è´¨é¢„æµ‹GNNæ¨¡å‹"""
    
    def __init__(self, num_features, num_classes, hidden_dim=64):
        super().__init__()
        self.conv1 = GCNConv(num_features, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)
        
        # å›¾çº§è¡¨ç¤º
        self.pool = global_mean_pool
        
        # åˆ†ç±»å™¨
        self.classifier = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        
        # å›¾å·ç§¯å±‚
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        
        # å›¾çº§æ± åŒ–
        x = self.pool(x, batch)
        
        # åˆ†ç±»
        x = self.classifier(x)
        return F.log_softmax(x, dim=1)

def molecular_property_experiment():
    """åˆ†å­æ€§è´¨é¢„æµ‹å®éªŒ"""
    # åŠ è½½æ•°æ®é›†
    dataset = TUDataset(root='data/MUTAG', name='MUTAG')
    
    # äº¤å‰éªŒè¯
    cv_scores = []
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(range(len(dataset)), 
                                                           [data.y.item() for data in dataset])):
        print(f"Fold {fold + 1}/10")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = [dataset[i] for i in train_idx]
        test_dataset = [dataset[i] for i in test_idx]
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = MolecularGNN(
            num_features=dataset.num_node_features,
            num_classes=dataset.num_classes
        )
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        
        # è®­ç»ƒ
        model.train()
        for epoch in range(100):
            total_loss = 0
            for batch in train_loader:
                optimizer.zero_grad()
                out = model(batch)
                loss = F.nll_loss(out, batch.y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
        
        # æµ‹è¯•
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch in test_loader:
                out = model(batch)
                pred = out.argmax(dim=1)
                correct += (pred == batch.y).sum().item()
                total += batch.y.size(0)
        
        accuracy = correct / total
        cv_scores.append(accuracy)
        print(f"Fold {fold + 1} Accuracy: {accuracy:.4f}")
    
    print(f"Average Accuracy: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")

if __name__ == "__main__":
    molecular_property_experiment()
```

#### ç¬¬31å‘¨ï¼šé“¾æ¥é¢„æµ‹ - æ¨èç³»ç»Ÿ
**é¡¹ç›®ç›®æ ‡**: åŸºäºç”¨æˆ·-å•†å“äº¤äº’å›¾è¿›è¡Œå•†å“æ¨è

```python
# recommendation_system.py
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, SAGEConv
from torch_geometric.utils import negative_sampling
import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score, average_precision_score

class RecommendationGNN(torch.nn.Module):
    """æ¨èç³»ç»ŸGNNæ¨¡å‹"""
    
    def __init__(self, num_users, num_items, embedding_dim=64):
        super().__init__()
        
        # ç”¨æˆ·å’Œå•†å“åµŒå…¥
        self.user_embedding = torch.nn.Embedding(num_users, embedding_dim)
        self.item_embedding = torch.nn.Embedding(num_items, embedding_dim)
        
        # å›¾å·ç§¯å±‚
        self.conv1 = SAGEConv(embedding_dim, embedding_dim)
        self.conv2 = SAGEConv(embedding_dim, embedding_dim)
        
        # é¢„æµ‹å±‚
        self.predictor = torch.nn.Sequential(
            torch.nn.Linear(embedding_dim * 2, embedding_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embedding_dim, 1),
            torch.nn.Sigmoid()
        )
        
    def forward(self, user_item_edge_index, user_indices, item_indices):
        # è·å–èŠ‚ç‚¹åµŒå…¥
        num_users = self.user_embedding.num_embeddings
        num_items = self.item_embedding.num_embeddings
        
        # åˆ›å»ºèŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
        x = torch.cat([
            self.user_embedding.weight,
            self.item_embedding.weight
        ], dim=0)
        
        # å›¾å·ç§¯
        x = F.relu(self.conv1(x, user_item_edge_index))
        x = F.relu(self.conv2(x, user_item_edge_index))
        
        # è·å–ç”¨æˆ·å’Œå•†å“è¡¨ç¤º
        user_repr = x[user_indices]
        item_repr = x[item_indices + num_users]  # å•†å“ç´¢å¼•åç§»
        
        # é¢„æµ‹äº¤äº’æ¦‚ç‡
        edge_repr = torch.cat([user_repr, item_repr], dim=1)
        return self.predictor(edge_repr).squeeze()

class RecommendationDataset:
    """æ¨èç³»ç»Ÿæ•°æ®é›†"""
    
    def __init__(self, ratings_file):
        self.ratings = pd.read_csv(ratings_file)
        self.prepare_data()
        
    def prepare_data(self):
        """å‡†å¤‡æ•°æ®"""
        # åˆ›å»ºç”¨æˆ·å’Œå•†å“IDæ˜ å°„
        self.user_to_idx = {user: idx for idx, user in enumerate(self.ratings['user_id'].unique())}
        self.item_to_idx = {item: idx for idx, item in enumerate(self.ratings['item_id'].unique())}
        
        self.num_users = len(self.user_to_idx)
        self.num_items = len(self.item_to_idx)
        
        # è½¬æ¢ä¸ºç´¢å¼•
        self.ratings['user_idx'] = self.ratings['user_id'].map(self.user_to_idx)
        self.ratings['item_idx'] = self.ratings['item_id'].map(self.item_to_idx)
        
        # åˆ›å»ºè¾¹ç´¢å¼•ï¼ˆç”¨æˆ·-å•†å“äº¤äº’ï¼‰
        user_indices = self.ratings['user_idx'].values
        item_indices = self.ratings['item_idx'].values + self.num_users  # å•†å“ç´¢å¼•åç§»
        
        self.edge_index = torch.tensor([
            np.concatenate([user_indices, item_indices]),
            np.concatenate([item_indices, user_indices])
        ], dtype=torch.long)
        
        # æ­£æ ·æœ¬è¾¹
        self.pos_edge_index = torch.tensor([user_indices, self.ratings['item_idx'].values], dtype=torch.long)
        
    def get_train_test_split(self, test_ratio=0.2):
        """åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†"""
        num_edges = self.pos_edge_index.size(1)
        indices = torch.randperm(num_edges)
        
        test_size = int(num_edges * test_ratio)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
        
        train_edge_index = self.pos_edge_index[:, train_indices]
        test_edge_index = self.pos_edge_index[:, test_indices]
        
        return train_edge_index, test_edge_index

def train_recommendation_model():
    """è®­ç»ƒæ¨èæ¨¡å‹"""
    # åŠ è½½æ•°æ®
    dataset = RecommendationDataset('data/ratings.csv')
    train_edge_index, test_edge_index = dataset.get_train_test_split()
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = RecommendationGNN(dataset.num_users, dataset.num_items)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(200):
        optimizer.zero_grad()
        
        # æ­£æ ·æœ¬
        pos_user_indices = train_edge_index[0]
        pos_item_indices = train_edge_index[1]
        pos_pred = model(dataset.edge_index, pos_user_indices, pos_item_indices)
        
        # è´Ÿæ ·æœ¬
        neg_edge_index = negative_sampling(
            train_edge_index, num_nodes=(dataset.num_users + dataset.num_items),
            num_neg_samples=train_edge_index.size(1)
        )
        neg_user_indices = neg_edge_index[0]
        neg_item_indices = neg_edge_index[1] - dataset.num_users  # ç§»é™¤åç§»
        neg_pred = model(dataset.edge_index, neg_user_indices, neg_item_indices)
        
        # æŸå¤±è®¡ç®—
        pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))
        neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))
        loss = pos_loss + neg_loss
        
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')
    
    # æµ‹è¯•è¯„ä¼°
    model.eval()
    with torch.no_grad():
        # æ­£æ ·æœ¬é¢„æµ‹
        test_pos_pred = model(dataset.edge_index, test_edge_index[0], test_edge_index[1])
        
        # è´Ÿæ ·æœ¬é¢„æµ‹
        test_neg_edge_index = negative_sampling(
            test_edge_index, num_nodes=(dataset.num_users + dataset.num_items),
            num_neg_samples=test_edge_index.size(1)
        )
        test_neg_pred = model(
            dataset.edge_index, 
            test_neg_edge_index[0], 
            test_neg_edge_index[1] - dataset.num_users
        )
        
        # è¯„ä¼°æŒ‡æ ‡
        y_true = torch.cat([torch.ones_like(test_pos_pred), torch.zeros_like(test_neg_pred)])
        y_score = torch.cat([test_pos_pred, test_neg_pred])
        
        auc = roc_auc_score(y_true.cpu(), y_score.cpu())
        ap = average_precision_score(y_true.cpu(), y_score.cpu())
        
        print(f'Test AUC: {auc:.4f}, Test AP: {ap:.4f}')

if __name__ == "__main__":
    train_recommendation_model()
```

### ç¬¬32-34å‘¨ï¼šçœŸå®ä¸–ç•Œåº”ç”¨é¡¹ç›®

#### æ™ºèƒ½æ¨èç³»ç»Ÿé¡¹ç›®
**é¡¹ç›®æè¿°**: æ„å»ºåŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¤šæ¨¡æ€æ¨èç³»ç»Ÿ

**æŠ€æœ¯ç‰¹ç‚¹**:
- å¼‚æ„å›¾å»ºæ¨¡ï¼ˆç”¨æˆ·-å•†å“-ç±»åˆ«-å“ç‰Œï¼‰
- å¤šæ¨¡æ€ç‰¹å¾èåˆï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¡Œä¸ºï¼‰
- å®æ—¶æ¨èæœåŠ¡éƒ¨ç½²
- A/Bæµ‹è¯•æ¡†æ¶

#### é‡‘èé£æ§é¡¹ç›®
**é¡¹ç›®æè¿°**: åŸºäºäº¤æ˜“ç½‘ç»œçš„åæ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿ

**æŠ€æœ¯ç‰¹ç‚¹**:
- åŠ¨æ€å›¾å»ºæ¨¡ï¼ˆæ—¶åºäº¤æ˜“ç½‘ç»œï¼‰
- å¼‚å¸¸æ£€æµ‹ç®—æ³•
- å¯è§£é‡Šæ€§åˆ†æ
- å®æ—¶é£é™©è¯„ä¼°

#### æ™ºæ…§åŸå¸‚é¡¹ç›®
**é¡¹ç›®æè¿°**: åŸå¸‚äº¤é€šæµé‡é¢„æµ‹å’Œä¼˜åŒ–

**æŠ€æœ¯ç‰¹ç‚¹**:
- æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ
- å¤šæºæ•°æ®èåˆ
- å®æ—¶é¢„æµ‹ç³»ç»Ÿ
- å¯è§†åŒ–å±•ç¤º

## ğŸ“Š é¡¹ç›®è¯„ä¼°æ ‡å‡†

### æŠ€æœ¯è¯„ä¼°
- **æ¨¡å‹æ€§èƒ½**: å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- **è®¡ç®—æ•ˆç‡**: è®­ç»ƒæ—¶é—´ã€æ¨ç†é€Ÿåº¦ã€å†…å­˜ä½¿ç”¨
- **å¯æ‰©å±•æ€§**: å¤§è§„æ¨¡æ•°æ®å¤„ç†èƒ½åŠ›
- **é²æ£’æ€§**: å¯¹å™ªå£°å’Œå¼‚å¸¸æ•°æ®çš„å¤„ç†èƒ½åŠ›

### å·¥ç¨‹è¯„ä¼°
- **ä»£ç è´¨é‡**: å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§ã€æµ‹è¯•è¦†ç›–ç‡
- **ç³»ç»Ÿè®¾è®¡**: æ¶æ„åˆç†æ€§ã€æ¨¡å—åŒ–ç¨‹åº¦
- **éƒ¨ç½²èƒ½åŠ›**: ç”Ÿäº§ç¯å¢ƒé€‚åº”æ€§
- **æ–‡æ¡£å®Œæ•´æ€§**: ä½¿ç”¨è¯´æ˜ã€APIæ–‡æ¡£ã€æŠ€æœ¯æŠ¥å‘Š

### ä¸šåŠ¡è¯„ä¼°
- **é—®é¢˜è§£å†³**: å®é™…é—®é¢˜çš„è§£å†³ç¨‹åº¦
- **ç”¨æˆ·ä½“éªŒ**: ç³»ç»Ÿæ˜“ç”¨æ€§ã€å“åº”é€Ÿåº¦
- **å•†ä¸šä»·å€¼**: æˆæœ¬æ•ˆç›Šã€ROIåˆ†æ
- **åˆ›æ–°æ€§**: æŠ€æœ¯åˆ›æ–°å’Œä¸šåŠ¡åˆ›æ–°

## ğŸ› ï¸ é¡¹ç›®å¼€å‘å·¥å…·

### å¼€å‘ç¯å¢ƒ
```bash
# åˆ›å»ºé¡¹ç›®ç¯å¢ƒ
conda create -n gnn-projects python=3.9
conda activate gnn-projects

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio
pip install torch-geometric
pip install dgl
pip install pandas numpy matplotlib seaborn
pip install scikit-learn
pip install jupyter notebook
pip install flask fastapi
pip install docker
```

### é¡¹ç›®æ¨¡æ¿
```
project_name/
â”œâ”€â”€ data/                   # æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ raw/               # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/         # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ external/          # å¤–éƒ¨æ•°æ®
â”œâ”€â”€ src/                   # æºä»£ç 
â”‚   â”œâ”€â”€ models/            # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ utils/             # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ preprocessing/     # æ•°æ®é¢„å¤„ç†
â”‚   â””â”€â”€ evaluation/        # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ notebooks/             # Jupyterç¬”è®°æœ¬
â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶
â”œâ”€â”€ tests/                # æµ‹è¯•ä»£ç 
â”œâ”€â”€ docker/               # Dockeræ–‡ä»¶
â”œâ”€â”€ docs/                 # æ–‡æ¡£
â”œâ”€â”€ requirements.txt      # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ setup.py             # å®‰è£…è„šæœ¬
â””â”€â”€ README.md            # é¡¹ç›®è¯´æ˜
```

## ğŸ¯ å­¦ä¹ æ£€æŸ¥ç‚¹

### é¡¹ç›®å¼€å‘èƒ½åŠ›
- [ ] èƒ½å¤Ÿç‹¬ç«‹å®Œæˆç«¯åˆ°ç«¯çš„GNNé¡¹ç›®
- [ ] æŒæ¡æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹æŠ€èƒ½
- [ ] å…·å¤‡æ¨¡å‹è°ƒä¼˜å’Œæ€§èƒ½ä¼˜åŒ–èƒ½åŠ›
- [ ] èƒ½å¤Ÿè¿›è¡Œå…¨é¢çš„å®éªŒè¯„ä¼°

### å·¥ç¨‹å®è·µèƒ½åŠ›
- [ ] éµå¾ªè½¯ä»¶å¼€å‘æœ€ä½³å®è·µ
- [ ] å…·å¤‡ç³»ç»Ÿè®¾è®¡å’Œæ¶æ„èƒ½åŠ›
- [ ] æŒæ¡æ¨¡å‹éƒ¨ç½²å’ŒæœåŠ¡åŒ–æŠ€èƒ½
- [ ] èƒ½å¤Ÿè¿›è¡Œé¡¹ç›®ç®¡ç†å’Œåä½œå¼€å‘

### é—®é¢˜è§£å†³èƒ½åŠ›
- [ ] èƒ½å¤Ÿåˆ†æå’Œå®šä¹‰å®é™…é—®é¢˜
- [ ] å…·å¤‡é€‰æ‹©åˆé€‚æŠ€æœ¯æ–¹æ¡ˆçš„èƒ½åŠ›
- [ ] èƒ½å¤Ÿå¤„ç†é¡¹ç›®ä¸­çš„å„ç§æŒ‘æˆ˜
- [ ] å…·å¤‡æŒç»­æ”¹è¿›å’Œåˆ›æ–°çš„æ„è¯†

é€šè¿‡è¿™äº›é¡¹ç›®å®è·µï¼Œä½ å°†ç§¯ç´¯ä¸°å¯Œçš„å®æˆ˜ç»éªŒï¼Œå…·å¤‡ç‹¬ç«‹å¼€å‘GNNåº”ç”¨ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„èŒä¸šå‘å±•å¥ å®šåšå®åŸºç¡€ï¼
