# ğŸ“š å­¦ä¹ èµ„æº (Resources)

æ±‡é›†å›¾ç¥ç»ç½‘ç»œå­¦ä¹ è¿‡ç¨‹ä¸­éœ€è¦çš„å„ç§èµ„æºï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€å·¥å…·ã€ä¹¦ç±è¯¾ç¨‹ç­‰ã€‚

## ğŸ¯ èµ„æºç›®æ ‡

- æä¾›é«˜è´¨é‡çš„å­¦ä¹ æ•°æ®é›†
- æ•´ç†å®ç”¨çš„å¼€å‘å·¥å…·
- æ¨èä¼˜ç§€çš„ä¹¦ç±å’Œè¯¾ç¨‹
- å»ºç«‹å®Œæ•´çš„èµ„æºåº“

## ğŸ“ ç›®å½•ç»“æ„

### ğŸ“Š datasets/
å¸¸ç”¨æ•°æ®é›†ï¼ŒåŒ…æ‹¬ï¼š
- **èŠ‚ç‚¹åˆ†ç±»æ•°æ®é›†**ï¼šCora, CiteSeer, PubMedç­‰
- **å›¾åˆ†ç±»æ•°æ®é›†**ï¼šMUTAG, PROTEINS, IMDBç­‰
- **é“¾æ¥é¢„æµ‹æ•°æ®é›†**ï¼šFacebook, Twitter, Amazonç­‰
- **çœŸå®ä¸–ç•Œæ•°æ®é›†**ï¼šäº¤é€šã€ç”Ÿç‰©ã€ç¤¾äº¤ç½‘ç»œæ•°æ®

### ğŸ› ï¸ tools/
å¼€å‘å·¥å…·é›†ï¼ŒåŒ…æ‹¬ï¼š
- **å¯è§†åŒ–å·¥å…·**ï¼šç½‘ç»œå¯è§†åŒ–ã€ç»“æœå±•ç¤º
- **æ•°æ®å¤„ç†å·¥å…·**ï¼šæ•°æ®æ¸…æ´—ã€æ ¼å¼è½¬æ¢
- **å®éªŒå·¥å…·**ï¼šè¶…å‚æ•°è°ƒä¼˜ã€ç»“æœç®¡ç†
- **éƒ¨ç½²å·¥å…·**ï¼šæ¨¡å‹æœåŠ¡åŒ–ã€å®¹å™¨åŒ–

### ğŸ“– books_courses/
ä¹¦ç±è¯¾ç¨‹æ¨èï¼ŒåŒ…æ‹¬ï¼š
- **ç»å…¸æ•™æ**ï¼šå›¾è®ºã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ 
- **ä¸“ä¸šä¹¦ç±**ï¼šå›¾ç¥ç»ç½‘ç»œä¸“è‘—
- **åœ¨çº¿è¯¾ç¨‹**ï¼šå¤§å­¦è¯¾ç¨‹ã€åœ¨çº¿æ•™ç¨‹
- **å­¦ä¹ è·¯å¾„**ï¼šç³»ç»Ÿå­¦ä¹ è§„åˆ’

## ğŸ“Š æ•°æ®é›†èµ„æº

### èŠ‚ç‚¹åˆ†ç±»æ•°æ®é›†

#### å¼•æ–‡ç½‘ç»œæ•°æ®é›†
```python
# citation_networks.py - å¼•æ–‡ç½‘ç»œæ•°æ®é›†åŠ è½½
from torch_geometric.datasets import Planetoid
import torch_geometric.transforms as T

class CitationDatasets:
    """å¼•æ–‡ç½‘ç»œæ•°æ®é›†ç®¡ç†"""
    
    @staticmethod
    def load_cora():
        """åŠ è½½Coraæ•°æ®é›†"""
        dataset = Planetoid(root='data/Cora', name='Cora', 
                          transform=T.NormalizeFeatures())
        return dataset
        
    @staticmethod
    def load_citeseer():
        """åŠ è½½CiteSeeræ•°æ®é›†"""
        dataset = Planetoid(root='data/CiteSeer', name='CiteSeer',
                          transform=T.NormalizeFeatures())
        return dataset
        
    @staticmethod
    def load_pubmed():
        """åŠ è½½PubMedæ•°æ®é›†"""
        dataset = Planetoid(root='data/PubMed', name='PubMed',
                          transform=T.NormalizeFeatures())
        return dataset
    
    @staticmethod
    def get_dataset_info():
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        datasets = ['Cora', 'CiteSeer', 'PubMed']
        info = {}
        
        for name in datasets:
            dataset = Planetoid(root=f'data/{name}', name=name)
            data = dataset[0]
            
            info[name] = {
                'num_nodes': data.x.size(0),
                'num_edges': data.edge_index.size(1) // 2,
                'num_features': data.x.size(1),
                'num_classes': len(torch.unique(data.y)),
                'train_nodes': data.train_mask.sum().item(),
                'val_nodes': data.val_mask.sum().item(),
                'test_nodes': data.test_mask.sum().item()
            }
        
        return info

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    info = CitationDatasets.get_dataset_info()
    for name, stats in info.items():
        print(f"{name}: {stats}")
```

#### ç¤¾äº¤ç½‘ç»œæ•°æ®é›†
```python
# social_networks.py - ç¤¾äº¤ç½‘ç»œæ•°æ®é›†
import networkx as nx
import pandas as pd
import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx

class SocialNetworkDatasets:
    """ç¤¾äº¤ç½‘ç»œæ•°æ®é›†ç®¡ç†"""
    
    @staticmethod
    def load_karate_club():
        """åŠ è½½ç©ºæ‰‹é“ä¿±ä¹éƒ¨æ•°æ®é›†"""
        G = nx.karate_club_graph()
        # æ·»åŠ èŠ‚ç‚¹ç‰¹å¾ï¼ˆåº¦æ•°ä½œä¸ºç‰¹å¾ï¼‰
        for node in G.nodes():
            G.nodes[node]['x'] = G.degree(node)
        
        data = from_networkx(G)
        return data
    
    @staticmethod
    def load_facebook_pages():
        """åŠ è½½Facebooké¡µé¢æ•°æ®é›†"""
        # è¿™é‡Œéœ€è¦ä¸‹è½½Facebooké¡µé¢æ•°æ®é›†
        edges = pd.read_csv('data/facebook_pages/edges.csv')
        features = pd.read_csv('data/facebook_pages/features.csv')
        labels = pd.read_csv('data/facebook_pages/labels.csv')
        
        edge_index = torch.tensor(edges[['source', 'target']].values.T, dtype=torch.long)
        x = torch.tensor(features.drop('page_id', axis=1).values, dtype=torch.float)
        y = torch.tensor(labels['page_type'].values, dtype=torch.long)
        
        return Data(x=x, edge_index=edge_index, y=y)
    
    @staticmethod
    def create_synthetic_social_network(num_users=1000, num_features=50):
        """åˆ›å»ºåˆæˆç¤¾äº¤ç½‘ç»œæ•°æ®"""
        # ä½¿ç”¨éšæœºå›¾æ¨¡å‹
        G = nx.barabasi_albert_graph(num_users, 3)
        
        # ç”Ÿæˆç”¨æˆ·ç‰¹å¾
        features = torch.randn(num_users, num_features)
        
        # ç”Ÿæˆæ ‡ç­¾ï¼ˆåŸºäºç¤¾åŒºç»“æ„ï¼‰
        communities = nx.community.greedy_modularity_communities(G)
        labels = torch.zeros(num_users, dtype=torch.long)
        for i, community in enumerate(communities):
            for node in community:
                labels[node] = i
        
        edge_index = torch.tensor(list(G.edges())).T
        
        return Data(x=features, edge_index=edge_index, y=labels)
```

### å›¾åˆ†ç±»æ•°æ®é›†

#### ç”Ÿç‰©åŒ–å­¦æ•°æ®é›†
```python
# biochemical_datasets.py - ç”Ÿç‰©åŒ–å­¦æ•°æ®é›†
from torch_geometric.datasets import TUDataset
import torch_geometric.transforms as T

class BiochemicalDatasets:
    """ç”Ÿç‰©åŒ–å­¦æ•°æ®é›†ç®¡ç†"""
    
    @staticmethod
    def load_mutag():
        """åŠ è½½MUTAGæ•°æ®é›†ï¼ˆåˆ†å­è‡´çªå˜æ€§ï¼‰"""
        dataset = TUDataset(root='data/MUTAG', name='MUTAG')
        return dataset
    
    @staticmethod
    def load_proteins():
        """åŠ è½½PROTEINSæ•°æ®é›†ï¼ˆè›‹ç™½è´¨åŠŸèƒ½ï¼‰"""
        dataset = TUDataset(root='data/PROTEINS', name='PROTEINS')
        return dataset
    
    @staticmethod
    def load_nci1():
        """åŠ è½½NCI1æ•°æ®é›†ï¼ˆæŠ—ç™Œæ´»æ€§ï¼‰"""
        dataset = TUDataset(root='data/NCI1', name='NCI1')
        return dataset
    
    @staticmethod
    def load_tox21():
        """åŠ è½½Tox21æ•°æ®é›†ï¼ˆæ¯’æ€§é¢„æµ‹ï¼‰"""
        # éœ€è¦å•ç‹¬ä¸‹è½½Tox21æ•°æ®é›†
        pass
    
    @staticmethod
    def get_molecular_datasets_info():
        """è·å–åˆ†å­æ•°æ®é›†ä¿¡æ¯"""
        dataset_names = ['MUTAG', 'PROTEINS', 'NCI1', 'PTC_MR']
        info = {}
        
        for name in dataset_names:
            try:
                dataset = TUDataset(root=f'data/{name}', name=name)
                info[name] = {
                    'num_graphs': len(dataset),
                    'num_classes': dataset.num_classes,
                    'num_node_features': dataset.num_node_features,
                    'num_edge_features': dataset.num_edge_features,
                    'avg_nodes': sum([data.x.size(0) for data in dataset]) / len(dataset),
                    'avg_edges': sum([data.edge_index.size(1) for data in dataset]) / len(dataset)
                }
            except:
                print(f"Failed to load dataset: {name}")
        
        return info
```

### é“¾æ¥é¢„æµ‹æ•°æ®é›†

```python
# link_prediction_datasets.py - é“¾æ¥é¢„æµ‹æ•°æ®é›†
import torch
import pandas as pd
import networkx as nx
from torch_geometric.data import Data
from torch_geometric.utils import negative_sampling, train_test_split_edges

class LinkPredictionDatasets:
    """é“¾æ¥é¢„æµ‹æ•°æ®é›†ç®¡ç†"""
    
    @staticmethod
    def load_facebook_social_network():
        """åŠ è½½Facebookç¤¾äº¤ç½‘ç»œ"""
        # ä¸‹è½½Facebookç¤¾äº¤ç½‘ç»œæ•°æ®
        edges = pd.read_csv('data/facebook_social/edges.txt', sep=' ', header=None)
        edge_index = torch.tensor(edges.values.T, dtype=torch.long)
        
        num_nodes = edge_index.max().item() + 1
        x = torch.eye(num_nodes)  # ä½¿ç”¨å•ä½çŸ©é˜µä½œä¸ºèŠ‚ç‚¹ç‰¹å¾
        
        data = Data(x=x, edge_index=edge_index)
        return data
    
    @staticmethod
    def load_protein_interaction():
        """åŠ è½½è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œ"""
        # åŠ è½½è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®
        ppi_data = pd.read_csv('data/ppi/interactions.csv')
        
        # æ„å»ºèŠ‚ç‚¹IDæ˜ å°„
        proteins = set(ppi_data['protein1'].unique()) | set(ppi_data['protein2'].unique())
        protein_to_id = {protein: i for i, protein in enumerate(proteins)}
        
        # æ„å»ºè¾¹ç´¢å¼•
        edges = []
        for _, row in ppi_data.iterrows():
            p1_id = protein_to_id[row['protein1']]
            p2_id = protein_to_id[row['protein2']]
            edges.append([p1_id, p2_id])
            edges.append([p2_id, p1_id])  # æ— å‘å›¾
        
        edge_index = torch.tensor(edges).T
        
        # ç”ŸæˆèŠ‚ç‚¹ç‰¹å¾ï¼ˆå¯ä»¥ä½¿ç”¨è›‹ç™½è´¨åºåˆ—ç‰¹å¾ç­‰ï¼‰
        num_nodes = len(proteins)
        x = torch.randn(num_nodes, 128)  # å‡è®¾ä½¿ç”¨128ç»´ç‰¹å¾
        
        return Data(x=x, edge_index=edge_index)
    
    @staticmethod
    def create_train_test_split(data, val_ratio=0.05, test_ratio=0.1):
        """åˆ›å»ºè®­ç»ƒéªŒè¯æµ‹è¯•é›†åˆ’åˆ†"""
        transform = train_test_split_edges(val_ratio=val_ratio, test_ratio=test_ratio)
        return transform(data)
    
    @staticmethod
    def load_amazon_product_network():
        """åŠ è½½Amazonå•†å“ç½‘ç»œ"""
        # åŠ è½½Amazonå•†å“å…±åŒè´­ä¹°ç½‘ç»œ
        edges = pd.read_csv('data/amazon/edges.txt', sep='\t', header=None)
        edge_index = torch.tensor(edges.values.T, dtype=torch.long)
        
        # åŠ è½½å•†å“ç‰¹å¾
        features = pd.read_csv('data/amazon/features.csv')
        x = torch.tensor(features.drop('product_id', axis=1).values, dtype=torch.float)
        
        return Data(x=x, edge_index=edge_index)
```

## ğŸ› ï¸ å¼€å‘å·¥å…·

### å¯è§†åŒ–å·¥å…·

```python
# visualization_tools.py - å¯è§†åŒ–å·¥å…·
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import torch
from torch_geometric.utils import to_networkx
import plotly.graph_objects as go
import plotly.express as px

class GraphVisualizer:
    """å›¾å¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def plot_graph_2d(data, node_labels=None, title="Graph Visualization"):
        """2Då›¾å¯è§†åŒ–"""
        G = to_networkx(data, to_undirected=True)
        
        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(G)
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(G, pos, alpha=0.6, edge_color='gray')
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        if node_labels is not None and hasattr(data, 'y'):
            unique_labels = torch.unique(data.y)
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = data.y == label
                node_list = [j for j, m in enumerate(mask) if m]
                nx.draw_networkx_nodes(G, pos, nodelist=node_list, 
                                     node_color=[colors[i]], 
                                     label=f'Class {label.item()}')
            plt.legend()
        else:
            nx.draw_networkx_nodes(G, pos, node_color='lightblue')
        
        plt.title(title)
        plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def plot_graph_3d_interactive(data, node_labels=None):
        """3Däº¤äº’å¼å›¾å¯è§†åŒ–"""
        G = to_networkx(data, to_undirected=True)
        pos = nx.spring_layout(G, dim=3)
        
        # æå–èŠ‚ç‚¹åæ ‡
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        node_z = [pos[node][2] for node in G.nodes()]
        
        # æå–è¾¹åæ ‡
        edge_x, edge_y, edge_z = [], [], []
        for edge in G.edges():
            x0, y0, z0 = pos[edge[0]]
            x1, y1, z1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            edge_z.extend([z0, z1, None])
        
        # åˆ›å»ºè¾¹çš„è½¨è¿¹
        edge_trace = go.Scatter3d(
            x=edge_x, y=edge_y, z=edge_z,
            mode='lines',
            line=dict(color='gray', width=2),
            hoverinfo='none'
        )
        
        # åˆ›å»ºèŠ‚ç‚¹çš„è½¨è¿¹
        node_trace = go.Scatter3d(
            x=node_x, y=node_y, z=node_z,
            mode='markers',
            marker=dict(
                size=8,
                color=data.y if hasattr(data, 'y') else 'lightblue',
                colorscale='Viridis',
                showscale=True
            ),
            text=[f'Node {i}' for i in range(len(node_x))],
            hoverinfo='text'
        )
        
        # åˆ›å»ºå›¾å½¢
        fig = go.Figure(data=[edge_trace, node_trace])
        fig.update_layout(
            title="3D Graph Visualization",
            showlegend=False,
            scene=dict(
                xaxis_title="X",
                yaxis_title="Y",
                zaxis_title="Z"
            )
        )
        
        fig.show()
    
    @staticmethod
    def plot_attention_weights(data, attention_weights, layer_idx=0, head_idx=0):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
        G = to_networkx(data, to_undirected=True)
        pos = nx.spring_layout(G)
        
        plt.figure(figsize=(12, 8))
        
        # ç»˜åˆ¶è¾¹ï¼Œè¾¹çš„ç²—ç»†è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡
        edges = list(G.edges())
        edge_weights = attention_weights[layer_idx][head_idx].detach().cpu().numpy()
        
        for i, (u, v) in enumerate(edges):
            if i < len(edge_weights):
                plt.plot([pos[u][0], pos[v][0]], [pos[u][1], pos[v][1]], 
                        'gray', alpha=0.6, linewidth=edge_weights[i] * 5)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=300)
        nx.draw_networkx_labels(G, pos, font_size=8)
        
        plt.title(f"Attention Weights - Layer {layer_idx}, Head {head_idx}")
        plt.axis('off')
        plt.tight_layout()
        plt.show()

class LearningCurveVisualizer:
    """å­¦ä¹ æ›²çº¿å¯è§†åŒ–"""
    
    @staticmethod
    def plot_training_curves(history):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(history['train_loss'], label='Train Loss')
        if 'val_loss' in history:
            ax1.plot(history['val_loss'], label='Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        if 'train_acc' in history:
            ax2.plot(history['train_acc'], label='Train Accuracy')
        if 'val_acc' in history:
            ax2.plot(history['val_acc'], label='Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def plot_hyperparameter_search(results):
        """å¯è§†åŒ–è¶…å‚æ•°æœç´¢ç»“æœ"""
        # å‡è®¾resultsæ˜¯åŒ…å«è¶…å‚æ•°å’Œæ€§èƒ½çš„å­—å…¸åˆ—è¡¨
        df = pd.DataFrame(results)
        
        # åˆ›å»ºè¶…å‚æ•°vsæ€§èƒ½çš„æ•£ç‚¹å›¾
        fig = px.scatter_matrix(df, dimensions=df.columns[:-1], 
                               color=df.columns[-1],
                               title="Hyperparameter Search Results")
        fig.show()
```

### å®éªŒç®¡ç†å·¥å…·

```python
# experiment_manager.py - å®éªŒç®¡ç†å·¥å…·
import json
import os
import pickle
import time
from datetime import datetime
import torch
import numpy as np
from typing import Dict, Any

class ExperimentManager:
    """å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, experiment_dir="experiments"):
        self.experiment_dir = experiment_dir
        os.makedirs(experiment_dir, exist_ok=True)
        
    def create_experiment(self, name: str, config: Dict[str, Any]) -> str:
        """åˆ›å»ºæ–°å®éªŒ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_id = f"{name}_{timestamp}"
        
        experiment_path = os.path.join(self.experiment_dir, experiment_id)
        os.makedirs(experiment_path, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(experiment_path, "config.json")
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        return experiment_id
    
    def log_metrics(self, experiment_id: str, metrics: Dict[str, float], step: int):
        """è®°å½•æŒ‡æ ‡"""
        experiment_path = os.path.join(self.experiment_dir, experiment_id)
        metrics_path = os.path.join(experiment_path, "metrics.jsonl")
        
        log_entry = {
            "step": step,
            "timestamp": time.time(),
            **metrics
        }
        
        with open(metrics_path, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
    
    def save_model(self, experiment_id: str, model: torch.nn.Module, epoch: int):
        """ä¿å­˜æ¨¡å‹"""
        experiment_path = os.path.join(self.experiment_dir, experiment_id)
        model_path = os.path.join(experiment_path, f"model_epoch_{epoch}.pth")
        torch.save(model.state_dict(), model_path)
    
    def save_results(self, experiment_id: str, results: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        experiment_path = os.path.join(self.experiment_dir, experiment_id)
        results_path = os.path.join(experiment_path, "results.json")
        
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
    
    def load_experiment(self, experiment_id: str) -> Dict[str, Any]:
        """åŠ è½½å®éªŒ"""
        experiment_path = os.path.join(self.experiment_dir, experiment_id)
        
        # åŠ è½½é…ç½®
        config_path = os.path.join(experiment_path, "config.json")
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # åŠ è½½æŒ‡æ ‡
        metrics_path = os.path.join(experiment_path, "metrics.jsonl")
        metrics = []
        if os.path.exists(metrics_path):
            with open(metrics_path, 'r') as f:
                for line in f:
                    metrics.append(json.loads(line))
        
        # åŠ è½½ç»“æœ
        results_path = os.path.join(experiment_path, "results.json")
        results = {}
        if os.path.exists(results_path):
            with open(results_path, 'r') as f:
                results = json.load(f)
        
        return {
            "config": config,
            "metrics": metrics,
            "results": results
        }
    
    def compare_experiments(self, experiment_ids: list) -> pd.DataFrame:
        """æ¯”è¾ƒå¤šä¸ªå®éªŒ"""
        comparison_data = []
        
        for exp_id in experiment_ids:
            exp_data = self.load_experiment(exp_id)
            
            row = {"experiment_id": exp_id}
            row.update(exp_data["config"])
            
            # æ·»åŠ æœ€ç»ˆç»“æœ
            if exp_data["results"]:
                row.update(exp_data["results"])
            
            # æ·»åŠ æœ€ä½³æŒ‡æ ‡
            if exp_data["metrics"]:
                metrics_df = pd.DataFrame(exp_data["metrics"])
                for col in metrics_df.columns:
                    if col not in ["step", "timestamp"]:
                        row[f"best_{col}"] = metrics_df[col].max()
            
            comparison_data.append(row)
        
        return pd.DataFrame(comparison_data)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    exp_manager = ExperimentManager()
    
    # åˆ›å»ºå®éªŒ
    config = {
        "model": "GCN",
        "hidden_dim": 64,
        "learning_rate": 0.01,
        "epochs": 200
    }
    exp_id = exp_manager.create_experiment("node_classification", config)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for epoch in range(10):
        metrics = {
            "train_loss": np.random.random(),
            "val_acc": np.random.random()
        }
        exp_manager.log_metrics(exp_id, metrics, epoch)
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    results = {"test_accuracy": 0.85, "test_f1": 0.83}
    exp_manager.save_results(exp_id, results)
```

## ğŸ“– æ¨èä¹¦ç±å’Œè¯¾ç¨‹

### ç»å…¸æ•™æ
1. **ã€Šæ·±åº¦å­¦ä¹ ã€‹** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
   - æ·±åº¦å­¦ä¹ çš„ç»å…¸æ•™æ
   - ç³»ç»Ÿä»‹ç»äº†æ·±åº¦å­¦ä¹ çš„ç†è®ºåŸºç¡€

2. **ã€Šå›¾è®ºå¯¼å¼•ã€‹** - Douglas B. West
   - å›¾è®ºçš„å…¥é—¨æ•™æ
   - æ¶µç›–å›¾è®ºçš„åŸºæœ¬æ¦‚å¿µå’Œç®—æ³•

3. **ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹** - æèˆª
   - æœºå™¨å­¦ä¹ çš„ç»å…¸ä¸­æ–‡æ•™æ
   - ç†è®ºä¸¥è°¨ï¼Œé€‚åˆæ·±å…¥å­¦ä¹ 

### ä¸“ä¸šä¹¦ç±
1. **ã€ŠGraph Representation Learningã€‹** - William L. Hamilton
   - å›¾è¡¨ç¤ºå­¦ä¹ çš„ä¸“é—¨æ•™æ
   - ä»åŸºç¡€åˆ°å‰æ²¿çš„å…¨é¢ä»‹ç»

2. **ã€ŠNetworks, Crowds, and Marketsã€‹** - David Easley, Jon Kleinberg
   - ç½‘ç»œåˆ†æçš„ç»å…¸æ•™æ
   - ç»“åˆç»æµå­¦å’Œç¤¾ä¼šå­¦è§†è§’

### åœ¨çº¿è¯¾ç¨‹
1. **CS224W: Machine Learning with Graphs** (Stanford)
   - Jure Leskovecæ•™æˆä¸»è®²
   - æœ€æƒå¨çš„å›¾æœºå™¨å­¦ä¹ è¯¾ç¨‹

2. **Deep Learning Specialization** (Coursera)
   - Andrew Ngæ•™æˆä¸»è®²
   - æ·±åº¦å­¦ä¹ çš„å…¥é—¨è¯¾ç¨‹

3. **CS231n: Convolutional Neural Networks** (Stanford)
   - è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ åŸºç¡€

### è®ºæ–‡å’Œç»¼è¿°
1. **Graph Neural Networks: A Review of Methods and Applications**
   - å›¾ç¥ç»ç½‘ç»œçš„å…¨é¢ç»¼è¿°
   - é€‚åˆå…¥é—¨å’Œå‚è€ƒ

2. **A Comprehensive Survey on Graph Neural Networks**
   - å¦ä¸€ç¯‡é‡è¦çš„GNNç»¼è¿°
   - æ›´æ–°æ›´åŠæ—¶

## ğŸ¯ å­¦ä¹ è·¯å¾„è§„åˆ’

### åˆå­¦è€…è·¯å¾„ (3-6ä¸ªæœˆ)
1. **æ•°å­¦åŸºç¡€** (4å‘¨)
   - çº¿æ€§ä»£æ•°å¤ä¹ 
   - æ¦‚ç‡è®ºåŸºç¡€
   - å¾®ç§¯åˆ†åŸºç¡€

2. **å›¾è®ºåŸºç¡€** (4å‘¨)
   - å›¾çš„åŸºæœ¬æ¦‚å¿µ
   - å›¾ç®—æ³•
   - NetworkXå®è·µ

3. **æ·±åº¦å­¦ä¹ åŸºç¡€** (6å‘¨)
   - ç¥ç»ç½‘ç»œåŸç†
   - PyTorchåŸºç¡€
   - ç»å…¸æ·±åº¦å­¦ä¹ æ¨¡å‹

4. **GNNå…¥é—¨** (6å‘¨)
   - GNNåŸºæœ¬æ¦‚å¿µ
   - ç»å…¸æ¨¡å‹å®ç°
   - æ¡†æ¶ä½¿ç”¨

### è¿›é˜¶è·¯å¾„ (6-12ä¸ªæœˆ)
1. **ç†è®ºæ·±å…¥** (8å‘¨)
   - GNNç†è®ºåˆ†æ
   - é«˜çº§æ¶æ„
   - æœ€æ–°ç ”ç©¶

2. **åº”ç”¨å®è·µ** (12å‘¨)
   - é¡¹ç›®å¼€å‘
   - çœŸå®æ•°æ®å¤„ç†
   - ç³»ç»Ÿéƒ¨ç½²

3. **ç ”ç©¶æ–¹å‘** (æŒç»­)
   - è®ºæ–‡é˜…è¯»
   - åˆ›æ–°ç ”ç©¶
   - å­¦æœ¯å†™ä½œ

### ä¸“å®¶è·¯å¾„ (12ä¸ªæœˆä»¥ä¸Š)
1. **æ·±åº¦ç ”ç©¶** 
   - åŸåˆ›æ€§ç ”ç©¶
   - å­¦æœ¯è®ºæ–‡å‘è¡¨
   - å¼€æºè´¡çŒ®

2. **æŠ€æœ¯é¢†å¯¼**
   - å›¢é˜Ÿåä½œ
   - æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡
   - è¡Œä¸šåº”ç”¨

## ğŸ”— åœ¨çº¿èµ„æºé“¾æ¥

### æ•°æ®é›†ä¸‹è½½
- [Stanford Large Network Dataset Collection](http://snap.stanford.edu/data/)
- [Graph Classification Datasets](https://chrsmrrs.github.io/datasets/)
- [Open Graph Benchmark](https://ogb.stanford.edu/)

### å·¥å…·å’Œæ¡†æ¶
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)
- [Deep Graph Library](https://www.dgl.ai/)
- [NetworkX](https://networkx.org/)

### å­¦æœ¯èµ„æº
- [Papers with Code - Graph Neural Networks](https://paperswithcode.com/methods/category/graph-neural-networks)
- [Awesome Graph Neural Networks](https://github.com/thunlp/GNNPapers)
- [Graph Deep Learning](https://github.com/gordicaleksa/pytorch-GAT)

å»ºç«‹è¿™ä¸ªå®Œæ•´çš„èµ„æºåº“å°†ä¸ºä½ çš„GNNå­¦ä¹ ä¹‹æ—…æä¾›å¼ºæœ‰åŠ›çš„æ”¯æŒï¼
