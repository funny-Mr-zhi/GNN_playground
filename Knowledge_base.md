> 记录知识关系图谱和跨学科碰撞新启发
# 知识图谱
> mermaid可能需要下载插件才能渲染
```mermaid
flowchart TB;
A[线性代数];
B[神经网络];
C[深度学习框架];
D[硬件加速];
E[图论];
F[GNN框架];
G[应用：电网];
A-->B;
B-->C
D-->C
D-->F
E-->F
C-->F
A-->E
A-->G
E-->G
```

# 交叉理解


## 前向理解
> 定义：前向理解：A-->B，A为B的基础铺垫知识，通过深入学习A对B有更好的理解

### 01. Linear Algebra for Linear Layer of NN

从线性代数行列变换的角度看神经网络中的线性层

* 线性代数矩阵乘法，可以理解为右侧矩阵行的线性组合或左侧列的线性组合
* 线性层计算：$A = XW^T + B$，主要启发在于矩阵乘法$X W^T$的理解
* 其它涉及的知识
    * 深度学习并行性主要来源
    * 线性层的理解，并引发关于非线性引入方式的思考

以三维feat输入为例
```
对单样本，一维输出，对x的三个feat(三列)进行简单线性组合: 得到标量a
                |w11|
|x11 x12 x13| * |w12| = a
                |w13|

接着，我们可能希望捕捉不止一种线性组合结果，扩增方法是增加W维度
                |w11 w12|
|x11 x12 x13| * |w12 w22| = |a1 a2|
                |w13 w23|

再进一步,我们希望同时处理多个样本，扩增方法是增加x的维度
|x11 x12 x13|   |w11 w13|   |a11 a12|
|x21 x22 x23| * |w12 w23| = |a21 a22|
|x31 x32 x33|   |w13 w33|   |a31 a32|

最终就得到了线性层的通用表示形式XWT
```

**启发**
* $XW^T = A$中，每一个$a_{ij}$都代表了样本i的第j种线性组合
* 进一步，结果矩阵A中，每一个元素的计算过程都是独立的，可以实现并行，也就是将串行的三层循环矩阵乘转化为一层循环，循环完成对样本所有特征的线性组合
    * 并行性一部分来自于右侧w的第二个维度，代表了独立的不同线性组合
    * 并行性一部分来自于左侧x的第一个维度，代表了独立的不同样本
    * 那么原理是只要有足够多的进程池，W的第二个维度和batch_size都不会一个单独的Linear Layer的计算速度。
    * **这事实上也带来了一种新的看待矩阵乘法的角度（后继知识反哺前置知识）**
* 线性层就是线性代数研究的线线性组合，对原样本特征进行多种线性组合，得到新的特征组合，有点类似于特征工程里的特征重组（替代了手动特征工程过程）。
    * 那么就引入新的问题，NN的非线性如何引入，激活函数是如何实现非线性的？（TODO）

### 02. Graph representation(Graph Theory) for GNN frameworks

> 图论中基础的图表示方法有邻接矩阵和邻接矩阵等，而PYG和DGL使用看起来完全图表示方法，这里深入探究PYG使用的(COO-Coordinate Format)和DGL使用的CSR(Compressed Sparse Row)/CSC(Compressed Sparse Column)格式。
>
> **随后在理解框架图表示方法的基础上深入探究GNN框架针对图做硬件优化的部分**

#### 三种表示方法的示例说明

> COO，CSR，CSC都是邻接矩阵的一种表示方式，可以直接对照着邻接矩阵来理解。
```
对于4x4的稀疏矩阵(带权重),假设邻接矩阵为
    |0 3 0 0|
A = |2 0 0 5|
    |0 0 1 0|
    |0 4 0 2|
```
**COO**

COO是最直观的稀疏矩阵表示方法，直接存储非零元素的坐标(行、列)和值。
e.g.
```
其COO格式表示为
row = [0, 1, 1, 2, 3, 3]
col = [1, 0 ,3, 2, 1, 3]
data= [3, 2, 5, 1, 4, 2]    # 三个数组长度相同，索引对应
```

**CSR/CSC**

CSR是一种行压缩格式，相应的CSC是一种列压缩格式，通过三个数组表示稀疏矩阵。
e.g.
```
CSR:
data  =   [3, 2, 5, 1 ,4, 2]  # 非零元素（行优先）
indices = [1, 0, 3, 2, 1, 3]  # 列索引  与非零元素一一对应
indptr = [0, 1, 3, 4, 6]  # 行指针：第i行的元素范围是 indptr[i]到indptr[i+1]-1
# indptr长度为n+1，相当于划分数据到n个行里。第i个数值代表第i行第一个非零元素的索引

CSC:
data    =  [2, 3, 4, 1, 5, 2]
indices =  [1, 0, 3, 2, 1, 3]   # 列索引，与非零元素一一对应
indptr = [0, 1, 3, 4, 6]
```
> 所以三种方法本质都是基础的邻接矩阵的变种，适合于稀疏矩阵（实际图大多是都是稀疏矩阵）

三种方式对比(节点数为n,边数为m)
格式|COO|CSR|CSC
--|--|--|--
存储方式|坐标三元组(i,j,v)|二元组(v, j)+行首索引|二元组(v, i)+列首索引
优势操作|构建简单，适合动态插入和任意顺序访问|高效行访问，适合逐行便利运算(矩阵-向量乘法)|高效列访问，适合按列进行的运算(矩阵转置，列聚合)
劣势操作|不适合直接进行矩阵运算(需转为CSR/CSC)。空间效率低|列访问效率低，插入、删除元素复杂|行访问效率低...
适用场景|快速构建稀疏矩阵|消息传递、图遍历(按节点)|特殊运算(如线性代数求解)
空间复杂度|O(3m)|O(2m+n+1)|O(2m+n+1)

> 总结一下，对于(稀疏)邻接矩阵的表示，直接使用三元组表示边是最直接也最简单的想法（也就是COO方法，PYG使用这种方法）。进一步，可以**按行遍历或列遍历的顺序提前将边排好序**，并压缩行/列索引到行/列数，节省了空间的同时提高了行/列访问的效率(这就是CSR/CSC,DGL使用这种方法)。当然这样做带来的损失就是边的增删都需要再重新遍历并排序以后的存储数据，对于动态图来说这种存储方式的增删成本会较高。

#### PYG和DGL如何加速
> 上面已经对三种存储方式有了深入的理解，接下来看一下GNN框架是如何针对边上消息传递的过程进行加速处理的。

**PYG加速机制**

PYG加速核心主要依赖于COO格式的灵活性和Pytorch的向量化运算

* 批处理优化:伪图(Pseudo-Graph)构建。将多个(小)图所有节点和边拼接，避免了显示循环处理每个图
```python
from thrch_geometric.data import Batch

# 假设date_list包含多个Data对象(小图)
batch = Batch.from_data_list
```
* 将消息传递转化为`稀疏矩阵-张量乘法`，`Pytorch`的`torch.spmm`针对COO格式做了优化，避免显示遍历每条边
```python
# 消息传递核心操作:X' = D^(-1/2)·A·D^(-1/2) · X
adj_t = normalize(edge_index)  # 归一化邻接矩阵
out = torch.spmm(adj_t, x)  #稀疏矩阵乘法
```
* 内存优化：
    * In-place操作，在不影响结果的情况下直接修改输入张量，减少内存分配
    * 缓存机制，对于静态图，缓存预计算结果(如：上面代码的归一化)，避免重复计算

> TODO,torch.spmm优化细节

**DGL加速机制**

DGL加速核心主要依赖于CSR/CSC格式的高效矩阵运算和图计算专用优化

* 块级并行：DGL将图划分为多个块，支持并行处理。原理是利用CSR格式行压缩特性，并行处理多个节点的消息
> 解决大规模图的 “拆分计算” 问题，通过分块计算 + 跨块通信实现并行，核心是 “空间拆分 + 数据依赖管理”。

* 算子融合：DGL将多个连续的图操作合并为一个核函数，减少GPU内核启动和数据传输开销。
```python
# 自定义消息函数和聚合函数, 详细用法见03frameworks-dgl学习笔记部分
def message_func(edges):
    return {'msg': edges.src['h']}  # 从源节点传递特征
def reduce_func(nodes):
    return {'h': torch.mean(nodes.mailbox['msg'], dim=1)}  # 聚合邻居消息
# 并行应用消息传递
g.update_all(message_func, reduce_func) # 包括消息传递函数和规约函数
# 更新函数可以直接写，前两个函数用到了DGL内置方法，可以针对图操作进行加速
# 融合消息函数和聚合函数为单一操作

g.apply_edges(lambda edges: {'e': edges.src['h'] * edges.dst['h']}) # 边表示更新
```
> 解决单设备内的 “计算效率” 问题。GNN 的消息传递包含多步连续操作（如 “消息生成→聚合→更新”），每步操作会产生中间结果（如临时消息、聚合缓存），频繁读写这些中间数据会导致内存带宽瓶颈（尤其在 GPU 上，内存访问比计算慢得多）。算子融合的本质是将多步独立算子合并为一个 “融合算子”，减少中间数据的存储与读写，核心是 “时间 / 计算流优化”。

* 内存预取：DGL提前将特征加载到GPU缓存中，减少内存访问延迟
```python
g.prefetch_feature('h')
```
> 就是异步内存拷贝，在计算上一批次数据是提前准备下一批次的数据。
* 图分区与分布式计算：对于超大规模图，DGL支持将图划分为多个子图，分布在多台机器或GPU上。
* 异构计算优化：CPU负责图结构操作(如遍历边、分区),GPU负责张量计算(如特征聚合，矩阵乘法)

**对比**
格式选择原因

框架|图表示格式|核心考量
--|--|--
PYG|COO|灵活性优先，适合快速进行原型开发，与pytorch深度继承,利用torch.sparseAPI
DGL|CSR/CSC|计算效率优先：优化矩阵乘法和消息传递；支持大规模图：CSR适合分块处理

加速策略对比

策略|PYG|DGL
--|--|--
批处理方式| 合并为伪图(Batch)| 原生支持图批量
消息传递核心| 稀疏矩阵乘法torch.spmm|块级并行+算子融合
内存优化| In-place操作 | 预取+缓存复用
分布式支持| 手动实现|内置图分区和分布式训练

性能对比
图规模|操作|PYG时间|DGL时间
--|--|--|--
小图(1k节点)|消息传递|12ms|8ms
中等图(100k节点)|消息传递|250ms|120ms
大图(10M节点)|全图训练|需分块处理|4.2s

**总结**
> 从优化的角度看，GNN主要是需要以边为循环进行消息传递(用于更新节点或边表示),框架主要优化点也在这里。PYG利用了pytorch原生的稀疏矩阵乘法进行优化，而DGL做了更有针对性的专门优化, 包括对图操作的并行优化和大图分布式支持。

> DGL优化效果显著，PYG适合快速原型开发，且在小图上性能差的不会太多。对于工业级应用，追求极致的效率，或必须处理大图时，DGL更合适。另一个考量角度是图的拓扑连接变化频率，PYG的COO可以高效处理图结构变化，而DGL的CSR处理图结构变化效率较低。


### 03. Linear Algebra for Graph theory and Power System

> 线性代数中矩阵赋予物理意义后，利用矩阵运算和性质证明“图论”定理和电力网络平衡定律

#### 基础欧拉公式和KCL定律

> 矩阵可以和离散数学中的"图"建立对应关系。当图也可以和实际的物理网络图建立映射。这里以直流电力图为例，$G = \{N, E\}$，节点上有属性电势$X$，边上有属性电流$Y$。用邻接表$A_{E\times N}$表示G的拓扑连接

如电力网络
```mermaid
flowchart LR
A[1]-->B[2]
B-->C[3]
A-->C
A-->D[4]
C-->D
```
对应的邻接表为：
$$A_{E \times N} = \begin{bmatrix}
-1 & 1 & 0 & 0\\
0 & -1 & 1 & 0\\
-1 & 0 & 1 & 0\\
-1 & 0 & 0 & 1\\
0 & 0 & -1 & 1\\
\end{bmatrix}$$

---

**数学含义**：
* A中每一行对应一条边，边线性无关表示不构成Loop(这里的Loop不看边方向，连成圈即可),线性相关就一定构成Loop
* 对于矩阵A，秩Rank的含义是最大线性无关行/列的数量
* 当A代表图G的邻接表时，可得$Rank(A) = n - 1$，即$Rank(A) = \#Nodes - 1$
* 从Loop数量来看，$A^T$主元列对应的边组成的子图是树，在此基础上每增加一条自由列对应的边，一定对应增加一个Loop, 即$\#Loops = dim(N(A^T))$
* 综上$dim(N(A^T)) = m - Rank(A) \rightarrow \#Loops + \#Nodes - \#Edges = 1$，即欧拉公式

**物理含义**：
* 电势:$X_{N\times 1}$
* 电势差：$e_{E\times 1} = AX$
* 电流：$Y_{E\times 1} = Ce$，其中C表示物理常数电导
* 基尔霍夫电流定律(KCL): $A^Y = 0$，表示各节点流入电流总量等于流出电流总量
* 引入外部电流后: $A^TY = f_{N \times 1}$，f表示每个节点的外部电流
* 应用数学常见完整公式$A^TCAX = f$

#### 矩阵表示完整霍尔基夫定律

基尔霍夫定律包括两个部分：电流部分KCL和电压部分KVL。
* KCL用语言描述就是节点流入电流与流出电流之和为零：用$A^TY = 0$即可表示
* KVL用语言描述就是闭合回路各段电压和为零。

> 对于闭合回路电势和为零首先可以用$U = AX$计算出每条边上的电势差。该公式电压差天然满足KVL。每个节点电势是单值的，节点连边构成的闭合回路电势升降和必然为零。
>
> 简单说：只要边电压降由节点电势差决定（\(\mathbf{U} = \mathbf{A} \mathbf{X}\)），闭合回路的总电压降就一定为零 —— 这是节点电势 “单值性” 的必然结果，也是 KVL 的核心本质。

举个例子：假设节点$V_1, V_2, V_3$通过边$E_{12}, E_{23}, E_{31}$连接形成闭合回路，电势差和表示为$U_{21} + U_{32} + U_{13} = (X_2 - X_1) + (X_3 - X_2) + (X_1 - X_3)$，回路各段电势差自然相消，闭合回路电势差和为零。

> 从矩阵性质角度看，闭合回路可以和A的左零空间建立关联。
> 
> 对于更一般的回路(规定正方向)，假设K为对A的行的掩码，回路上与正方向相同的边掩码为1，相反的边掩码为-1，不在回路上掩码取0

```mermaid
flowchart LR
A[1]-->B[2]
B-->C[3]
A-->C
4;
```
对应的K为$[1, 1, -1， 0， 0]^T$, $K^TU$为0, 即$K^TAX = 0$

由于这个公式对于任意X都必须成立，即$K^TA = 0 \rightarrow A^TK = 0^T$，即K在A的左零空间中。(在线性代数Class12也已经证明， 通过闭合回路计算出来向量可以构成A的左零空间的一组基)

> 理解：对于A的左零空间组成，其实就是使得A的行的线性组合结果为0的一个行向量K，当K能使得行的线性组合结果为0时，就代表K指出的一系列边从一个点出发，最终又回到了原点，构成一个闭合回路。(当然实际选取闭合回路时，边的取值一定为0或正负1，实际的完整闭合回路一定对应A的左零空间向量，但A的左零空间向量可以是不同实际闭合回路的线性组合，组合后边的取值不一定还是0或正负1，没有对应到图上的实际闭合回路)

#### 矩阵表示交流霍尔基夫定律与潮流方程

#####  AC(交流电)电力网络背景

对于交流电路，电流和电压随时间周期性变化。其变化规律和三角函数有关，为了将复杂的三角函数表示和运算简化，采用“数学简化技巧”，用复数表示交流电中的电压和电流。

电压电流都是正弦变化的：$i(t) = I_msin(\omega t + \phi )$，直接用正弦计算KCL和KVL会面临两个问题，运算复杂且电压电流的相位差影响难以直接体现。而复数(向量)恰好能同时表示幅值和相位，还能把正弦量的加减转为为复数的加减，乘除转化为模的相乘，角相加，大大简化计算。

复数(向量)表示正弦量的核心是利用欧拉公式$e^{j\theta} = cos\theta + jsin\theta$

$$i(t) = I_msin(\omega t + \phi)\\ = I_mcos(\omega t + \phi - 90^。)\\ = Re[I_me_{j(\omega t + \phi - 90^。)}]$$

提取时间无关部分(这就是向量)
$\dot I = I_me^{j(\phi-90^。)}\\简化为\dot I = I\angle \phi$

> 取复数中实数部分就是电流的实际值。**简单来所，复数向量就像是给正弦量拍了一张快照，定格了它的幅值和相位，忽略重乎的时间变化部分，从而让计算更加高效**

###### 交流电下KCL和FVL

两个电流相加就是对应向量相加,电压同理

$$KCL-DC：i_1 + i_2 + \dots = 0 \\ KCL-AC: \dot I_1 + \dot I_2 + \dots = 0$$

$$KVL-AC: \dot U_1 + \dot U_2 + \dots = 0$$

**阻抗Z**(阻碍电阻的能力)：
* 实部-电阻R：阻碍电流并消耗电能的能力(影响电流大小, I = U/R)
* 虚部-电抗X：阻碍电流并存储电能的能力(影响电流变化快慢)
    * 正虚部-感抗XL：电感阻碍特定频率交流电的能力(电流变化滞后, $XL = 2\pi f L$)
    * 负虚部-容抗XC：电容阻碍特定频率交流电的能力(电流变化领先, $XC = \frac{1}{2\pi f C}$)
> 当XL和XC正好相互抵消时，无功功率互补，为零，实现了无功功率的补偿。

**导纳Y**：Z的倒数，用电导电纳表示$Y = G + jB$

交流电中：
$$Z = \frac{\dot U}{\dot I} \\ |Z| = \frac{U}{I}, \ \phi_Z = \phi_U - \phi_I$$

$$\dot I = Y · \dot U \\ |Y| = \frac{I}{U}, \  \phi_Y = \phi_I - \phi_U$$

> **符号问题**：I 和 U上面加点是向量符号的专用标记，该复数专门应用于某一正弦函数；而 Y 和 Z 只是复参数，而非向量，不对应某个随时间变化的正弦量。

###### 交流潮流方程 

类比直流电用矩阵表示的方法:
* 节点电压$\dot U_{N \times 1}$为复数电压列向量
* 节点电流$\dot I_{N \times 1}$为复数电流列向量
* 导纳矩阵$Y_{N \times N}$为复数导纳矩阵
* 邻接表仍用A表示

对单个节点i，KCL表示为$\dot I_i = \sum_{j=1}^n\dot I_{ij}$, 支路电流表示为$\dot I_{ij} = y_{ij}(\dot U_i - \dot U_j)$

对于导纳矩阵Y，取$Y_{ij} = -y_{ij}$，$Y_{ii} = \sum_{j=1}^ny_{ij}$时，通过$Y\dot U就可以直接还原上述的支路电流计算公式$

KCL矩阵表示形式为: $0 = A^T\dot I = A^T Y\dot U$

> 在在交流电路的 KCL 和 KVL 基础上推导节点潮流方程，核心是将 “电流平衡” 转化为 “功率平衡”

节点注入功率(复功率)$$~S_i = P_i + jQ_i \\ = \dot U_i · \dot I_i^* \\ =\dot U_i · \sum_{j=1}^n Y^*_{ij}\dot U_{j}^*$$

(共轭是因为复功率定义为$U·I^*$,确保有功功率为实数)

> 在计算注入复功率时，可以直接用$~S = P + jQ = \dot U Y^* \dot U^*$表示，但矩阵不利于化简，针对一个节点功率中的求和序列中的一项进行化简

$$\dot U_i Y_{ij} \dot U_j = U_i(cos\theta_i + sin\theta_i)(G_{ij} - jB_{ij})U_j(cos\theta_j - sin\theta_j) \\ 
= U_iU_j(G_{ij} - jB_{ij})[(cos\theta_i + sin\theta_i)(cos\theta_j - sin\theta_j)]\\
= U_iU_j(G_{ij} - jB_{ij})[cos(\theta_i-\theta_j) + jsin(\theta_i - \theta_j)]\\
= U_iU_j[(G_{ij}cos\theta_{ij} + B_{ij}sin\theta_{ij}) + j(G_{ij}sin\theta_{ij} - B_{ij}cos\theta_{ij})]$$

有功和无功平衡公式分别为：
* $ P_i = U_i\sum_{j=1}^{n}U_j[G_{ij}cos\theta_{ij} + B_{ij}sin\theta_{ij}]$
* $ Q_i = U_i\sum_{j=1}^{n}U_j[G_{ij}sin\theta_{ij} - B_{ij}cos\theta_{ij}]$


> 复功率S向量用矩阵表示:

针对上面单个项的简化结果，引入相角差矩阵$D_{N \times N}, \ \  d_{ij} = \theta_i = \theta_j$

中间表示:对应每条边上的系数的矩阵
$$\Phi = GcosD + BsinD \\ \Omega = GsinD - BcosD$$

左侧电压对应行，右侧电压对应列

$$P = U \Phi U \\ Q = U \Omega U$$

> 深入理解矩阵运算的基础上，可以将复杂的线性变换和求和计算转化成矩阵运算，书写更简洁。也便于后续在利用计算机运算时直接套用并行算法。









